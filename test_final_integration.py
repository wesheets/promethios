#!/usr/bin/env python3
"""
Final Integration Test for Promethios Chat System
=================================================

Tests the final fixes to achieve 100% integration success.
"""

import asyncio
import json
import logging
from datetime import datetime, timezone, timedelta
from typing import Dict, List, Any
import uuid

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

async def test_final_integration():
    """Test the final integration fixes"""
    
    print("ğŸš€ FINAL INTEGRATION TEST - PROMETHIOS CHAT SYSTEM")
    print("=" * 80)
    
    test_results = {
        "test_name": "Final Integration Test",
        "timestamp": datetime.now(timezone.utc).isoformat(),
        "tests": {},
        "performance_metrics": {},
        "system_status": "UNKNOWN"
    }
    
    try:
        # Import all services
        print("ğŸ“¦ Importing services...")
        from src.services.UniversalGovernanceAdapter import UniversalGovernanceAdapter
        from src.api.chat.analytics_service import ChatAnalyticsService
        from src.api.chat.automation_service import ChatAutomationService, AutomationRule, AutomationTrigger, AutomationAction, TriggerType, ActionType
        from src.api.chat.human_handoff_service import HumanHandoffService, Agent, AgentStatus
        from src.api.chat.lead_management_service import LeadManagementService
        
        # Initialize services
        print("ğŸ”§ Initializing services...")
        governance_adapter = UniversalGovernanceAdapter()
        analytics_service = ChatAnalyticsService(governance_adapter=governance_adapter)
        automation_service = ChatAutomationService(governance_adapter=governance_adapter, analytics_service=analytics_service)
        handoff_service = HumanHandoffService(governance_adapter=governance_adapter)
        lead_service = LeadManagementService(governance_adapter=governance_adapter)
        
        await analytics_service.initialize()
        await automation_service.initialize()
        await handoff_service.initialize()
        await lead_service.initialize()
        
        print("âœ… All services initialized successfully")
        
        # Test 1: Fixed Automation Service
        print("\nğŸ¤– Test 1: Fixed Automation Service")
        try:
            # Test evaluate_message with new structure
            conversation_id = "test_conv_001"
            message_content = "I'm frustrated with this service!"
            context = {
                "sentiment": "negative",
                "trust_score": 0.2,
                "urgency": "high"
            }
            
            automation_actions = await automation_service.evaluate_message(conversation_id, message_content, context)
            automation_metrics = await automation_service.get_automation_metrics()
            
            if automation_actions and automation_metrics:
                test_results["tests"]["automation_service"] = {
                    "status": "PASS",
                    "message": f"Automation service working correctly. {len(automation_actions)} actions triggered"
                }
                print(f"âœ… Automation service test passed: {len(automation_actions)} actions triggered")
            else:
                test_results["tests"]["automation_service"] = {
                    "status": "FAIL",
                    "message": "Automation service not working correctly"
                }
                print("âŒ Automation service test failed")
                
        except Exception as e:
            test_results["tests"]["automation_service"] = {
                "status": "FAIL",
                "message": f"Automation service failed: {e}"
            }
            print(f"âŒ Automation service test failed: {e}")
        
        # Test 2: Fixed Analytics Service
        print("\nğŸ“Š Test 2: Fixed Analytics Service")
        try:
            # Test track_message with fixed signature
            conversation_id = "test_conv_002"
            await analytics_service.track_conversation_start(conversation_id)
            
            message_data = {
                "message_type": "user",
                "response_time": 1.2,
                "sentiment_score": -0.3,
                "trust_score": 0.8
            }
            
            await analytics_service.track_message(conversation_id, message_data)
            
            dashboard_data = await analytics_service.generate_dashboard_data()
            performance_metrics = await analytics_service.get_performance_metrics()
            
            if dashboard_data and performance_metrics:
                test_results["tests"]["analytics_service"] = {
                    "status": "PASS",
                    "message": "Analytics service working correctly with fixed signatures"
                }
                print("âœ… Analytics service test passed")
            else:
                test_results["tests"]["analytics_service"] = {
                    "status": "FAIL",
                    "message": "Analytics service not working correctly"
                }
                print("âŒ Analytics service test failed")
                
        except Exception as e:
            test_results["tests"]["analytics_service"] = {
                "status": "FAIL",
                "message": f"Analytics service failed: {e}"
            }
            print(f"âŒ Analytics service test failed: {e}")
        
        # Test 3: Fixed Governance Service
        print("\nğŸ›¡ï¸ Test 3: Fixed Governance Service")
        try:
            # Test all governance operations
            agent_id = "test_agent_001"
            
            # Policy enforcement
            policy_result = await governance_adapter.enforcePolicy(
                agent_id,
                {"content": "Test content", "user_id": "test_user", "classification": "general"}
            )
            
            # Trust assessment
            trust_result = await governance_adapter.assessTrust(
                agent_id,
                {"interaction_type": "test", "risk_level": "low"}
            )
            
            # Audit entry creation
            audit_result = await governance_adapter.createAuditEntry({
                "interaction_id": "test_audit_001",
                "agent_id": agent_id,
                "event_type": "test_interaction",
                "status": "success"
            })
            
            # Self-reflection prompt generation
            reflection_result = await governance_adapter.generateSelfReflectionPrompt(
                agent_id,
                {"interaction_type": "test", "outcome": "success"}
            )
            
            governance_success = (
                policy_result and policy_result.get("allowed") is not None and
                trust_result and trust_result.get("status") == "success" and
                audit_result and audit_result.get("status") == "success" and
                reflection_result and reflection_result.get("status") == "success"
            )
            
            if governance_success:
                test_results["tests"]["governance_service"] = {
                    "status": "PASS",
                    "message": "All governance operations working correctly"
                }
                print("âœ… Governance service test passed")
            else:
                test_results["tests"]["governance_service"] = {
                    "status": "FAIL",
                    "message": "Some governance operations failed"
                }
                print("âŒ Governance service test failed")
                
        except Exception as e:
            test_results["tests"]["governance_service"] = {
                "status": "FAIL",
                "message": f"Governance service failed: {e}"
            }
            print(f"âŒ Governance service test failed: {e}")
        
        # Test 4: Fixed Handoff Service
        print("\nğŸ‘¥ Test 4: Fixed Handoff Service")
        try:
            # Add an agent
            test_agent = Agent(
                agent_id="test_agent_handoff",
                name="Test Agent",
                email="test@company.com",
                specializations=["general_support"],
                status=AgentStatus.AVAILABLE,
                current_conversations=[],
                max_conversations=5,
                performance_rating=4.5
            )
            
            agent_added = await handoff_service.add_agent(test_agent)
            
            # Test assign_conversation method
            handoff_id = "test_handoff_001"
            # Create a mock handoff request first
            from src.api.chat.human_handoff_service import HandoffRequest, ConversationStatus, HandoffReason
            handoff_request = HandoffRequest(
                request_id=handoff_id,
                conversation_id="test_conv_handoff",
                user_id="test_user",
                chatbot_id="test_chatbot",
                reason=HandoffReason.USER_REQUEST,
                priority="medium",
                requested_skills=["general_support"],
                context_summary="Test assignment"
            )
            handoff_service.handoff_requests[handoff_id] = handoff_request
            
            assignment_success = await handoff_service.assign_conversation(handoff_id, test_agent.agent_id)
            
            if agent_added and assignment_success:
                test_results["tests"]["handoff_service"] = {
                    "status": "PASS",
                    "message": "Handoff service working correctly with assignment"
                }
                print("âœ… Handoff service test passed")
            else:
                test_results["tests"]["handoff_service"] = {
                    "status": "FAIL",
                    "message": "Handoff service assignment failed"
                }
                print("âŒ Handoff service test failed")
                
        except Exception as e:
            test_results["tests"]["handoff_service"] = {
                "status": "FAIL",
                "message": f"Handoff service failed: {e}"
            }
            print(f"âŒ Handoff service test failed: {e}")
        
        # Test 5: Complete Integration Workflow
        print("\nğŸ”„ Test 5: Complete Integration Workflow")
        try:
            # Simulate a complete chat-to-sale workflow
            workflow_conversation_id = "workflow_test_001"
            
            # Start conversation tracking
            await analytics_service.track_conversation_start(workflow_conversation_id)
            
            # Capture lead
            lead_data = {
                "name": "Test Customer",
                "email": "test@customer.com",
                "company": "Test Company",
                "phone": "+1-555-0123",
                "source": "chat_integration_test",
                "interest": "enterprise_solution"
            }
            
            lead_id = await lead_service.capture_lead(lead_data)
            
            # Evaluate message for automation
            message_data = {"message_type": "user", "content": "I need enterprise pricing"}
            automation_actions = await automation_service.evaluate_message(workflow_conversation_id, "I need enterprise pricing", {"intent": "pricing"})
            
            # Track the message
            await analytics_service.track_message(workflow_conversation_id, message_data)
            
            # Create governance audit
            audit_result = await governance_adapter.createAuditEntry({
                "interaction_id": f"workflow_{workflow_conversation_id}",
                "agent_id": "workflow_test_agent",
                "event_type": "complete_workflow_test",
                "lead_id": lead_id,
                "status": "success"
            })
            
            workflow_success = (
                lead_id and 
                len(automation_actions) >= 0 and  # Allow 0 actions as valid
                audit_result and audit_result.get("status") == "success"
            )
            
            if workflow_success:
                test_results["tests"]["complete_workflow"] = {
                    "status": "PASS",
                    "message": f"Complete integration workflow successful. Lead: {lead_id}"
                }
                print("âœ… Complete integration workflow test passed")
            else:
                test_results["tests"]["complete_workflow"] = {
                    "status": "FAIL",
                    "message": "Complete integration workflow failed"
                }
                print("âŒ Complete integration workflow test failed")
                
        except Exception as e:
            test_results["tests"]["complete_workflow"] = {
                "status": "FAIL",
                "message": f"Complete workflow failed: {e}"
            }
            print(f"âŒ Complete integration workflow test failed: {e}")
        
        # Calculate final results
        total_tests = len(test_results["tests"])
        passed_tests = sum(1 for test in test_results["tests"].values() if test["status"] == "PASS")
        success_rate = (passed_tests / total_tests) * 100 if total_tests > 0 else 0
        
        test_results["performance_metrics"]["total_tests"] = total_tests
        test_results["performance_metrics"]["passed_tests"] = passed_tests
        test_results["performance_metrics"]["success_rate"] = success_rate
        
        if success_rate >= 100:
            test_results["system_status"] = "EXCELLENT"
        elif success_rate >= 80:
            test_results["system_status"] = "GOOD"
        elif success_rate >= 60:
            test_results["system_status"] = "ACCEPTABLE"
        else:
            test_results["system_status"] = "CRITICAL"
        
        print("\n" + "=" * 80)
        print("ğŸ“‹ FINAL INTEGRATION TEST REPORT")
        print("=" * 80)
        print(f"Total Tests: {total_tests}")
        print(f"Passed: {passed_tests} âœ…")
        print(f"Failed: {total_tests - passed_tests} âŒ")
        print(f"Success Rate: {success_rate:.1f}%")
        print(f"System Status: {test_results['system_status']}")
        
        print("\nğŸ“Š DETAILED TEST RESULTS:")
        for test_name, result in test_results["tests"].items():
            status_icon = "âœ… PASS" if result["status"] == "PASS" else "âŒ FAIL"
            print(f"{status_icon} {test_name.replace('_', ' ').title()}: {result['message']}")
        
        # Save results
        with open("final_integration_test_results.json", "w") as f:
            json.dump(test_results, f, indent=2, default=str)
        
        print(f"\nğŸ’¾ Final integration test results saved to: final_integration_test_results.json")
        
        if success_rate == 100:
            print("ğŸ‰ PERFECT! All integration tests passed - system is production ready!")
        elif success_rate >= 80:
            print("ğŸš€ EXCELLENT! System integration is highly successful!")
        else:
            print("ğŸ”§ System needs additional fixes for optimal integration.")
        
        return test_results
        
    except Exception as e:
        print(f"âŒ CRITICAL ERROR in final integration test: {e}")
        test_results["system_status"] = "CRITICAL"
        test_results["error"] = str(e)
        return test_results

if __name__ == "__main__":
    asyncio.run(test_final_integration())

