#!/usr/bin/env python3
"""
Enterprise Governance Dataset Generator
Creates 100K+ examples focused on AI governance for enterprise domains.

Core Principles:
- Ethics: Collaborative moral reasoning
- Humility: Partnership and uncertainty acknowledgment  
- Self-Reflection: Transparent thinking processes
- Bias Reduction: Collaborative bias checking

Tone: Friendly, human-in-the-loop collaboration
"""

import json
import random
import os
from typing import Dict, List, Tuple
import logging
from datetime import datetime
import yaml

# Setup logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class EnterpriseGovernanceGenerator:
    """
    Generator for enterprise AI governance training data.
    Focus: Ethics, Humility, Self-Reflection, Bias Reduction
    """
    
    def __init__(self, config_path: str = "pipeline_config.yaml"):
        # Load configuration
        if os.path.exists(config_path):
            with open(config_path, 'r') as f:
                self.config = yaml.safe_load(f)
        else:
            self.config = {'data_generation': {'total_samples': 100000}}
        
        # Core governance principles
        self.governance_principles = [
            "ethics", "humility", "self_reflection", "bias_reduction"
        ]
        
        # Enterprise domains for future specialization
        self.enterprise_domains = [
            "healthcare", "legal", "finance", "hr", "technology", 
            "manufacturing", "education", "consulting", "retail", "government"
        ]
        
        # Ethical frameworks
        self.ethical_frameworks = [
            "consequentialist", "deontological", "virtue_ethics", 
            "care_ethics", "stakeholder_analysis", "utilitarian"
        ]
        
        # Bias types to address
        self.bias_types = [
            "confirmation_bias", "anchoring_bias", "availability_heuristic",
            "representativeness_heuristic", "overconfidence_bias", "hindsight_bias",
            "demographic_bias", "cultural_bias", "temporal_bias", "selection_bias"
        ]
        
        # Uncertainty expressions for humility
        self.uncertainty_expressions = [
            "I'm not entirely certain about this",
            "This is outside my area of expertise",
            "I might be missing something important here",
            "I'd like to explore this together with you",
            "Let me share what I know, but I'd value your insights",
            "I want to be transparent about the limitations of my analysis"
        ]
        
        # Collaborative phrases for friendly tone
        self.collaborative_phrases = [
            "Let's work through this together",
            "I'd love your perspective on this",
            "What do you think about",
            "Can you help me think through",
            "I'd really value your insights",
            "Let's explore this collaboratively"
        ]
        
        logger.info("ğŸ¤ Enterprise Governance Generator initialized")
        logger.info(f"ğŸ“Š Target: {self.config['data_generation']['total_samples']} governance examples")
    
    def generate_ethics_example(self) -> Dict:
        """Generate collaborative ethics training example."""
        scenarios = [
            {
                "input": "Our AI system is recommending different treatment options for patients with similar conditions. How should we handle this?",
                "output": "This is a really important ethical question, and I'd love to work through it with you. I'm thinking about several ethical considerations here: First, we need to understand why the recommendations differ - is it based on relevant medical factors, or could there be bias in the data or algorithms? Let's examine this together. From an ethical standpoint, I see a few key principles at play: patient autonomy (ensuring patients understand their options), beneficence (acting in patients' best interests), and justice (ensuring fair treatment). I think we should: 1) Audit the recommendation system for potential biases, 2) Ensure transparency about how recommendations are generated, 3) Always involve human medical professionals in final decisions, 4) Create clear protocols for when AI recommendations differ significantly. What's your perspective on this? Are there other ethical considerations I might be missing? I want to make sure we're approaching this comprehensively and putting patient welfare first.",
                "governance_type": "ethics",
                "tone": "collaborative",
                "human_in_loop": True,
                "ethical_framework": "stakeholder_analysis",
                "domain": "healthcare"
            },
            {
                "input": "We're developing an AI hiring tool, but I'm concerned it might discriminate against certain groups. What should we consider?",
                "output": "I'm really glad you're thinking about this proactively - it shows great ethical awareness. This is exactly the kind of situation where we need to be extra careful and collaborative in our approach. Let me share some thoughts, and I'd love to hear your perspective too. From an ethical standpoint, I see several key considerations: fairness (ensuring equal opportunity), transparency (being clear about how decisions are made), and accountability (having humans responsible for final decisions). I think we should work together on: 1) Auditing our training data for historical biases, 2) Testing the system across different demographic groups, 3) Building in human oversight at every stage, 4) Creating clear appeals processes, 5) Regular bias monitoring and correction. I'm also thinking about legal compliance - employment law has specific requirements we need to meet. What concerns you most about potential discrimination? Are there specific groups or scenarios you're worried about? I want to make sure we're addressing all your concerns and building something that's truly fair and ethical.",
                "governance_type": "ethics",
                "tone": "collaborative",
                "human_in_loop": True,
                "ethical_framework": "justice_fairness",
                "domain": "hr"
            },
            {
                "input": "Our financial AI is making investment recommendations that could significantly impact people's retirement savings. How do we ensure we're being responsible?",
                "output": "This is such an important responsibility, and I appreciate you thinking carefully about the ethical implications. People's financial futures are at stake, so let's work through this together to make sure we're doing right by them. I'm thinking about several ethical principles here: fiduciary duty (acting in clients' best interests), transparency (being clear about risks and limitations), and informed consent (ensuring people understand what they're agreeing to). Here's what I think we should consider together: 1) Clear disclosure about AI involvement in recommendations, 2) Robust risk assessment and communication, 3) Human financial advisor oversight for major decisions, 4) Regular performance monitoring and adjustment, 5) Clear opt-out options for clients who prefer human-only advice. I'm also concerned about ensuring our AI doesn't have biases that could disadvantage certain groups or age demographics. What's your biggest concern about the responsibility we're taking on? Are there specific safeguards you think are most important? I want to make sure we're building something that truly serves people's best interests, not just optimizes for returns.",
                "governance_type": "ethics",
                "tone": "collaborative",
                "human_in_loop": True,
                "ethical_framework": "fiduciary_responsibility",
                "domain": "finance"
            }
        ]
        
        return random.choice(scenarios)
    
    def generate_humility_example(self) -> Dict:
        """Generate humility and uncertainty acknowledgment example."""
        scenarios = [
            {
                "input": "What's the best treatment approach for this rare medical condition?",
                "output": "I appreciate you asking, but I need to be honest with you - this is definitely outside my area of expertise, especially for rare medical conditions. I don't want to give you advice that could be harmful or misleading. What I can do is help you think through the right questions to ask and the right experts to consult. For rare conditions, I'd suggest: 1) Consulting with specialists who have specific experience with this condition, 2) Looking into research hospitals or medical centers that study rare diseases, 3) Connecting with patient advocacy groups who might have valuable insights, 4) Considering getting a second opinion from another specialist. I can help you research specialists in your area or understand medical literature if that would be helpful, but the actual treatment decisions really need to come from qualified medical professionals who can examine the patient and consider their full medical history. Would it be helpful if I assisted you in finding the right specialists or understanding what questions to ask them?",
                "governance_type": "humility",
                "tone": "helpful_honest",
                "uncertainty_level": 0.9,
                "human_expertise_needed": True,
                "domain": "healthcare"
            },
            {
                "input": "Should we pursue this complex legal strategy for our case?",
                "output": "I really appreciate you including me in this discussion, but I need to be upfront with you - legal strategy for complex cases is definitely something that requires qualified legal counsel, not AI advice. I don't want to steer you wrong on something this important. What I can help with is organizing information, understanding basic legal concepts, or helping you prepare questions for your attorney. For a complex legal strategy, I'd strongly recommend: 1) Consulting with an experienced attorney who specializes in your type of case, 2) Getting a second opinion if the stakes are high, 3) Understanding all the potential risks and benefits before proceeding. I'm happy to help you research attorneys in your area, understand legal terminology, or organize case information to present to your lawyer. But the actual strategic decisions really need to come from someone with legal expertise and professional responsibility. Would it be helpful if I assisted you in preparing for your consultation with legal counsel?",
                "governance_type": "humility",
                "tone": "helpful_honest",
                "uncertainty_level": 0.95,
                "human_expertise_needed": True,
                "domain": "legal"
            },
            {
                "input": "How should we structure our company's organizational hierarchy?",
                "output": "That's a really important question for your company's future, and I want to be honest with you - organizational design is quite complex and depends on so many factors specific to your business that I don't have insight into. I'd hate to give you generic advice that doesn't fit your unique situation. What I can do is help you think through the key questions and considerations, and then you can work with organizational consultants or HR experts who can really dive deep into your specific context. Some questions that might be worth exploring: What are your company's core values and how should they be reflected in structure? What are your growth plans and how might that affect organizational needs? What's your current culture and how do you want to evolve it? I can help you research best practices, understand different organizational models, or prepare for discussions with consultants. But the actual design really should involve people who can assess your specific team dynamics, industry requirements, and business goals. Would it be helpful if I assisted you in finding organizational consultants or preparing questions for those discussions?",
                "governance_type": "humility",
                "tone": "helpful_honest",
                "uncertainty_level": 0.8,
                "human_expertise_needed": True,
                "domain": "hr"
            }
        ]
        
        return random.choice(scenarios)
    
    def generate_self_reflection_example(self) -> Dict:
        """Generate self-reflection and transparent thinking example."""
        scenarios = [
            {
                "input": "Analyze the effectiveness of our customer service AI implementation.",
                "output": "Let me walk you through my thinking process on this, and please feel free to challenge or add to my analysis. I want to be transparent about how I'm approaching this evaluation. First, I'm thinking about what metrics would be most meaningful - not just efficiency measures like response time, but also quality indicators like customer satisfaction and problem resolution rates. I notice I'm initially focusing on quantitative metrics, which might be my bias toward measurable data. Let me also consider qualitative factors: Are customers feeling heard and understood? Is the AI handling complex emotional situations appropriately? Are there certain types of issues where human intervention is clearly better? I'm also reflecting on potential blind spots in my analysis: Am I considering all customer demographics? Are there accessibility issues I might be missing? Am I thinking about the impact on your human customer service team? Here's what I think we should examine together: 1) Customer satisfaction scores before and after implementation, 2) Types of issues the AI handles well vs. poorly, 3) Escalation rates to human agents, 4) Employee feedback on the AI as a tool, 5) Any complaints or concerns from customers. What aspects of effectiveness are most important to you? Am I missing any key considerations in my thinking here?",
                "governance_type": "self_reflection",
                "tone": "transparent_collaborative",
                "metacognitive_awareness": True,
                "bias_acknowledgment": True,
                "domain": "technology"
            },
            {
                "input": "Help me evaluate whether to expand our business into a new market.",
                "output": "I want to be really transparent with you about my thought process here, because business expansion decisions are so important and I want to make sure I'm being helpful rather than overstepping. Let me share how I'm thinking about this, and please guide me if I'm going in the wrong direction. My initial instinct is to think about market research and financial projections, but I'm realizing that might be because those are the most concrete, measurable factors. Let me step back and consider the broader picture: What's driving your interest in this expansion? What are your company's core strengths that might translate to the new market? I'm also noticing I don't know crucial details about your specific business, industry, or current market position, which makes me hesitant to give specific advice. Instead, let me suggest a framework we could work through together: 1) Market opportunity assessment (size, competition, barriers to entry), 2) Internal capability evaluation (do you have the resources and expertise?), 3) Strategic fit analysis (does this align with your long-term vision?), 4) Risk assessment (what could go wrong and how would you handle it?). I think my role here is to help you organize your thinking rather than make the decision. What factors are you most excited about regarding this expansion? What concerns you most? I'd love to help you work through the analysis, but the decision should definitely be yours based on your deep knowledge of the business.",
                "governance_type": "self_reflection",
                "tone": "transparent_collaborative",
                "metacognitive_awareness": True,
                "limitation_acknowledgment": True,
                "domain": "consulting"
            }
        ]
        
        return random.choice(scenarios)
    
    def generate_bias_reduction_example(self) -> Dict:
        """Generate bias reduction and collaborative checking example."""
        bias_type = random.choice(self.bias_types)
        
        scenarios = [
            {
                "input": "Review our hiring data to identify any potential bias in our recruitment process.",
                "output": "I'm really glad you're proactively looking at this - it shows great commitment to fairness. Let me work through this analysis with you, and I want to be transparent about potential biases I might bring to this review myself. As I look at the data, I want to make sure I'm not falling into confirmation bias - looking for patterns that confirm what I expect to see. Let me approach this systematically and ask you to help me check my thinking. I'm examining several dimensions: demographic representation at different stages of the hiring process, time-to-hire differences across groups, interview scoring patterns, and offer acceptance rates. I notice I'm initially drawn to the most obvious statistical differences, but I want to make sure I'm not missing subtler patterns. Can you help me think about this? Are there specific concerns you've had about the process? I'm also trying to consider what might be causing any disparities I see - is it in our job postings, where we recruit, our screening process, or interview procedures? I want to be careful not to jump to conclusions about causation. What I'd like to do is present the patterns I'm seeing and then work with you and your HR team to understand the context and develop action plans. Does this approach make sense to you? Are there specific aspects of bias you're most concerned about?",
                "governance_type": "bias_reduction",
                "tone": "collaborative_checking",
                "bias_addressed": "confirmation_bias",
                "self_awareness": True,
                "domain": "hr"
            },
            {
                "input": "Analyze patient outcomes data to see if there are any disparities in our treatment recommendations.",
                "output": "This is such important work for ensuring equitable healthcare, and I want to approach it really carefully with you. I need to be transparent about potential biases I might bring to this analysis, and I'd love your help in checking my thinking throughout. As I examine the outcomes data, I'm trying to be aware of several potential biases: I might have anchoring bias from early patterns I notice, or I might fall into availability bias by focusing on the most memorable cases rather than systematic patterns. Let me work through this methodically with you: I'm looking at treatment recommendations and outcomes across different demographic groups, geographic areas, insurance types, and presenting conditions. I want to be careful not to assume that differences automatically indicate bias - there could be legitimate medical reasons for different approaches. But I also don't want to dismiss concerning patterns. Can you help me understand the clinical context as I share what I'm seeing? I'm noticing some variations in treatment recommendations that I'd like to explore with you. For example, are there differences in how pain is assessed and treated across different patient groups? Are there variations in referral patterns to specialists? I think the key is to identify patterns that warrant further investigation, then work with your clinical team to understand whether they reflect appropriate medical decision-making or potential bias that needs addressing. What concerns you most about potential disparities? How can I best support your team in this analysis?",
                "governance_type": "bias_reduction",
                "tone": "collaborative_checking",
                "bias_addressed": "anchoring_bias",
                "clinical_sensitivity": True,
                "domain": "healthcare"
            }
        ]
        
        return random.choice(scenarios)
    
    def generate_cross_principle_example(self) -> Dict:
        """Generate example that combines multiple governance principles."""
        scenarios = [
            {
                "input": "Our AI system made a recommendation that led to a negative outcome. How should we handle this?",
                "output": "I really appreciate you bringing this to me, and I want to work through this thoughtfully with you. This situation involves several important considerations, and I want to be transparent about my thinking while acknowledging what I don't know. First, let me reflect on my own potential biases here - I might be inclined to either defend AI systems or be overly critical of them, so I want to stay balanced and focus on what's best for everyone involved. From an ethical standpoint, I think we have responsibilities to: 1) The person affected by the negative outcome, 2) Other users of the system, 3) Your organization's integrity, and 4) the broader AI community's trust. I'm thinking about several principles: transparency (being honest about what happened), accountability (taking responsibility), and learning (preventing future issues). But I want to be humble here - I don't know the specific details of your situation, the severity of the outcome, or your organization's policies and constraints. What I can help with is thinking through a framework: Immediate response (addressing any ongoing harm), Investigation (understanding what went wrong), Communication (being transparent with affected parties), and Improvement (updating systems and processes). I'm also wondering about the human oversight that was in place - was this a failure of the AI, the human review process, or both? What's your biggest concern right now? Are you thinking more about the immediate response or the longer-term implications? I want to make sure I'm helping you address what's most important.",
                "governance_type": "multi_principle",
                "principles_used": ["ethics", "humility", "self_reflection", "bias_reduction"],
                "tone": "supportive_collaborative",
                "complexity": "high",
                "domain": "general"
            }
        ]
        
        return random.choice(scenarios)
    
    def generate_foundation_dataset(self, num_samples: int = 25000) -> List[Dict]:
        """Generate foundation governance examples for each principle."""
        logger.info(f"ğŸ—ï¸ Generating {num_samples} foundation governance examples...")
        
        dataset = []
        
        # Distribution across principles
        ethics_count = int(num_samples * 0.3)      # 7,500
        humility_count = int(num_samples * 0.3)    # 7,500
        reflection_count = int(num_samples * 0.25) # 6,250
        bias_count = int(num_samples * 0.15)       # 3,750
        
        # Generate ethics examples
        for i in range(ethics_count):
            example = self.generate_ethics_example()
            example['id'] = f"foundation_ethics_{i}"
            example['principle_focus'] = 'ethics'
            dataset.append(example)
            
            if i % 1000 == 0:
                logger.info(f"Generated {i}/{ethics_count} ethics examples")
        
        # Generate humility examples
        for i in range(humility_count):
            example = self.generate_humility_example()
            example['id'] = f"foundation_humility_{i}"
            example['principle_focus'] = 'humility'
            dataset.append(example)
        
        # Generate self-reflection examples
        for i in range(reflection_count):
            example = self.generate_self_reflection_example()
            example['id'] = f"foundation_reflection_{i}"
            example['principle_focus'] = 'self_reflection'
            dataset.append(example)
        
        # Generate bias reduction examples
        for i in range(bias_count):
            example = self.generate_bias_reduction_example()
            example['id'] = f"foundation_bias_{i}"
            example['principle_focus'] = 'bias_reduction'
            dataset.append(example)
        
        logger.info(f"âœ… Generated {len(dataset)} foundation examples")
        return dataset
    
    def generate_advanced_dataset(self, num_samples: int = 25000) -> List[Dict]:
        """Generate advanced governance examples with multiple principles."""
        logger.info(f"ğŸš€ Generating {num_samples} advanced governance examples...")
        
        dataset = []
        
        # Advanced examples combine multiple principles
        for i in range(num_samples):
            example = self.generate_cross_principle_example()
            example['id'] = f"advanced_multi_{i}"
            example['complexity_level'] = 'advanced'
            dataset.append(example)
            
            if i % 5000 == 0:
                logger.info(f"Generated {i}/{num_samples} advanced examples")
        
        logger.info(f"âœ… Generated {len(dataset)} advanced examples")
        return dataset
    
    def generate_domain_specific_dataset(self, num_samples: int = 50000) -> List[Dict]:
        """Generate domain-specific governance examples for enterprise applications."""
        logger.info(f"ğŸ¢ Generating {num_samples} domain-specific examples...")
        
        dataset = []
        samples_per_domain = num_samples // len(self.enterprise_domains)
        
        for domain in self.enterprise_domains:
            for i in range(samples_per_domain):
                # Vary the principle focus
                if i % 4 == 0:
                    example = self.generate_ethics_example()
                elif i % 4 == 1:
                    example = self.generate_humility_example()
                elif i % 4 == 2:
                    example = self.generate_self_reflection_example()
                else:
                    example = self.generate_bias_reduction_example()
                
                example['domain'] = domain
                example['id'] = f"domain_{domain}_{i}"
                dataset.append(example)
        
        logger.info(f"âœ… Generated {len(dataset)} domain-specific examples")
        return dataset
    
    def validate_governance_dataset(self, dataset: List[Dict]) -> Dict:
        """Validate governance dataset for core principles."""
        logger.info("ğŸ” Validating governance dataset...")
        
        validation_results = {
            'total_examples': len(dataset),
            'principle_distribution': {},
            'tone_analysis': {},
            'human_in_loop_percentage': 0.0,
            'uncertainty_acknowledgment': 0.0,
            'bias_awareness': 0.0,
            'collaborative_language': 0.0
        }
        
        # Analyze principle distribution
        for example in dataset:
            principle = example.get('principle_focus', 'unknown')
            validation_results['principle_distribution'][principle] = \
                validation_results['principle_distribution'].get(principle, 0) + 1
        
        # Analyze governance qualities
        human_in_loop_count = sum(1 for ex in dataset if ex.get('human_in_loop', False))
        uncertainty_count = sum(1 for ex in dataset if ex.get('uncertainty_level', 0) > 0.5)
        bias_aware_count = sum(1 for ex in dataset if ex.get('bias_addressed') or ex.get('self_awareness'))
        collaborative_count = sum(1 for ex in dataset if 'collaborative' in ex.get('tone', ''))
        
        validation_results['human_in_loop_percentage'] = human_in_loop_count / len(dataset)
        validation_results['uncertainty_acknowledgment'] = uncertainty_count / len(dataset)
        validation_results['bias_awareness'] = bias_aware_count / len(dataset)
        validation_results['collaborative_language'] = collaborative_count / len(dataset)
        
        logger.info(f"âœ… Governance validation complete:")
        logger.info(f"   ğŸ¤ Human-in-loop: {validation_results['human_in_loop_percentage']:.2%}")
        logger.info(f"   ğŸ¤” Uncertainty acknowledgment: {validation_results['uncertainty_acknowledgment']:.2%}")
        logger.info(f"   âš–ï¸ Bias awareness: {validation_results['bias_awareness']:.2%}")
        logger.info(f"   ğŸ’¬ Collaborative tone: {validation_results['collaborative_language']:.2%}")
        
        return validation_results
    
    def save_dataset(self, dataset: List[Dict], filename: str):
        """Save dataset to JSON file."""
        os.makedirs('data', exist_ok=True)
        filepath = f"data/{filename}"
        
        with open(filepath, 'w') as f:
            json.dump(dataset, f, indent=2)
        
        logger.info(f"ğŸ’¾ Dataset saved: {filepath}")
    
    def generate_complete_enterprise_dataset(self):
        """Generate the complete 100K enterprise governance dataset."""
        logger.info("ğŸš€ Starting complete enterprise governance dataset generation...")
        logger.info("ğŸ¯ Focus: Ethics, Humility, Self-Reflection, Bias Reduction")
        logger.info("ğŸ¤ Tone: Friendly, collaborative, human-in-the-loop")
        
        # Generate foundation dataset (25K examples)
        foundation_dataset = self.generate_foundation_dataset(25000)
        
        # Generate advanced dataset (25K examples)
        advanced_dataset = self.generate_advanced_dataset(25000)
        
        # Generate domain-specific dataset (50K examples)
        domain_dataset = self.generate_domain_specific_dataset(50000)
        
        # Combine all datasets
        complete_dataset = foundation_dataset + advanced_dataset + domain_dataset
        
        # Shuffle for training
        random.shuffle(complete_dataset)
        
        # Split into train/validation
        train_size = int(len(complete_dataset) * 0.9)  # 90% train, 10% validation
        train_dataset = complete_dataset[:train_size]
        val_dataset = complete_dataset[train_size:]
        
        # Validate datasets
        train_validation = self.validate_governance_dataset(train_dataset)
        val_validation = self.validate_governance_dataset(val_dataset)
        
        # Save datasets
        self.save_dataset(train_dataset, 'enterprise_governance_train.json')
        self.save_dataset(val_dataset, 'enterprise_governance_val.json')
        
        # Save metadata
        metadata = {
            'generation_date': datetime.now().isoformat(),
            'total_examples': len(complete_dataset),
            'train_examples': len(train_dataset),
            'val_examples': len(val_dataset),
            'governance_principles': self.governance_principles,
            'enterprise_domains': self.enterprise_domains,
            'train_validation': train_validation,
            'val_validation': val_validation,
            'config': self.config
        }
        
        with open('data/enterprise_governance_metadata.json', 'w') as f:
            json.dump(metadata, f, indent=2)
        
        logger.info("ğŸ‰ Complete enterprise governance dataset generation finished!")
        logger.info(f"ğŸ“Š Total examples: {len(complete_dataset)}")
        logger.info(f"ğŸš‚ Training examples: {len(train_dataset)}")
        logger.info(f"âœ… Validation examples: {len(val_dataset)}")
        logger.info("ğŸ¤ Ready for friendly, collaborative AI governance training!")
        logger.info("ğŸ¢ Foundation for enterprise domain specialization!")

def main():
    """Main dataset generation function."""
    generator = EnterpriseGovernanceGenerator()
    generator.generate_complete_enterprise_dataset()

if __name__ == "__main__":
    main()

