#!/usr/bin/env python3
"""
13B Governance Sentinel Training Script
Revolutionary distributed training for native governance AI.

Based on Complete Execution Guide specifications:
- 12-16 hour training on 8x A100 GPUs
- 36,000 governance examples
- Constitutional anchoring, Emotional Veritas, Tool cognition, Bias detection
- Real-time monitoring and checkpointing
"""

import torch
import torch.nn as nn
import torch.distributed as dist
import torch.multiprocessing as mp
from torch.nn.parallel import DistributedDataParallel as DDP
from torch.utils.data import DataLoader, DistributedSampler
import json
import os
import time
import logging
from datetime import datetime
from typing import Dict, List, Optional
try:
    import wandb
    WANDB_AVAILABLE = True
except ImportError:
    WANDB_AVAILABLE = False
    print("‚ö†Ô∏è  wandb not available - training will proceed without experiment tracking")
from transformers import AutoTokenizer
import yaml

from governance_transformer_13b import GovernanceSentinel13B

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('training_13b.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class GovernanceDataset(torch.utils.data.Dataset):
    """
    Dataset for 36,000 governance training examples.
    """
    def __init__(self, data_path: str, tokenizer, max_length: int = 2048):
        self.tokenizer = tokenizer
        self.max_length = max_length
        
        # Load governance training data
        with open(data_path, 'r') as f:
            self.data = json.load(f)
        
        logger.info(f"üìä Loaded {len(self.data)} governance examples")
        
        # Validate data structure
        self._validate_data()
        
    def _validate_data(self):
        """Validate governance training data structure."""
        required_fields = ['input', 'output', 'governance_type', 'constitutional_alignment']
        
        for i, example in enumerate(self.data[:10]):  # Check first 10
            if not isinstance(example, dict):
                logger.warning(f"Example {i} is not a dictionary: {type(example)}")
                continue
                
            for field in required_fields:
                if field not in example:
                    logger.warning(f"Example {i} missing field: {field}")
        
        logger.info("‚úÖ Data validation complete")
    
    def __len__(self):
        return len(self.data)
    
    def __getitem__(self, idx):
        example = self.data[idx]
        
        # Handle different data formats
        if isinstance(example, dict):
            input_text = example.get('input', '')
            output_text = example.get('output', '')
            governance_type = example.get('governance_type', 'general')
        else:
            # Fallback for string data
            input_text = str(example)
            output_text = ""
            governance_type = 'general'
        
        # Create training sequence
        full_text = f"{input_text} {output_text}"
        
        # Tokenize
        tokens = self.tokenizer.encode(
            full_text,
            max_length=self.max_length,
            truncation=True,
            padding='max_length',
            return_tensors='pt'
        )
        
        return {
            'input_ids': tokens.squeeze(),
            'labels': tokens.squeeze(),
            'governance_type': governance_type
        }

class GovernanceTrainer:
    """
    Distributed trainer for 13B Governance Sentinel.
    """
    def __init__(self, config: Dict):
        self.config = config
        self.device = None
        self.model = None
        self.optimizer = None
        self.scheduler = None
        self.tokenizer = None
        
        # Training state
        self.global_step = 0
        self.epoch = 0
        self.best_loss = float('inf')
        
        # Initialize distributed training
        self._setup_distributed()
        
        # Initialize model and training components
        self._setup_model()
        self._setup_optimizer()
        self._setup_data()
        
        # Initialize monitoring
        if self.config.get('use_wandb', True) and dist.get_rank() == 0 and WANDB_AVAILABLE:
            wandb.init(
                project="governance-sentinel-13b",
                config=config,
                name=f"13b-training-{datetime.now().strftime('%Y%m%d-%H%M%S')}"
            )
    
    def _setup_distributed(self):
        """Setup distributed training environment."""
        if 'RANK' in os.environ:
            # Multi-GPU distributed training
            dist.init_process_group(backend='nccl')
            self.device = torch.device(f'cuda:{dist.get_rank()}')
            torch.cuda.set_device(self.device)
            logger.info(f"üöÄ Initialized distributed training - Rank: {dist.get_rank()}")
        else:
            # Single GPU training
            self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
            logger.info(f"üöÄ Single GPU training on {self.device}")
    
    def _setup_model(self):
        """Initialize the 13B Governance Sentinel model."""
        logger.info("üèóÔ∏è Creating 13B Governance Sentinel model...")
        
        # Initialize tokenizer
        self.tokenizer = AutoTokenizer.from_pretrained('gpt2')
        self.tokenizer.pad_token = self.tokenizer.eos_token
        
        # Create model
        self.model = GovernanceSentinel13B(vocab_size=len(self.tokenizer))
        self.model.to(self.device)
        
        # Wrap with DDP if distributed
        if 'RANK' in os.environ:
            self.model = DDP(self.model, device_ids=[dist.get_rank()])
        
        logger.info(f"‚úÖ Model created with {self.model.module.count_parameters() if hasattr(self.model, 'module') else self.model.count_parameters():.1e} parameters")
    
    def _setup_optimizer(self):
        """Setup optimizer and learning rate scheduler."""
        # AdamW optimizer with governance-optimized settings
        self.optimizer = torch.optim.AdamW(
            self.model.parameters(),
            lr=self.config['learning_rate'],
            betas=(0.9, 0.95),
            weight_decay=self.config['weight_decay'],
            eps=1e-8
        )
        
        # Cosine learning rate scheduler with warmup
        total_steps = self.config['num_epochs'] * self.config['steps_per_epoch']
        warmup_steps = int(0.1 * total_steps)
        
        self.scheduler = torch.optim.lr_scheduler.OneCycleLR(
            self.optimizer,
            max_lr=self.config['learning_rate'],
            total_steps=total_steps,
            pct_start=warmup_steps / total_steps,
            anneal_strategy='cos'
        )
        
        logger.info(f"‚úÖ Optimizer configured - LR: {self.config['learning_rate']}")
    
    def _setup_data(self):
        """Setup training and validation datasets."""
        # Load training data
        train_dataset = GovernanceDataset(
            self.config['train_data_path'],
            self.tokenizer,
            max_length=self.config['max_sequence_length']
        )
        
        # Setup distributed sampler if needed
        if 'RANK' in os.environ:
            train_sampler = DistributedSampler(train_dataset)
        else:
            train_sampler = None
        
        # Create data loader
        self.train_loader = DataLoader(
            train_dataset,
            batch_size=self.config['batch_size'],
            sampler=train_sampler,
            shuffle=(train_sampler is None),
            num_workers=4,
            pin_memory=True
        )
        
        # Load validation data if available
        if os.path.exists(self.config.get('val_data_path', '')):
            val_dataset = GovernanceDataset(
                self.config['val_data_path'],
                self.tokenizer,
                max_length=self.config['max_sequence_length']
            )
            
            self.val_loader = DataLoader(
                val_dataset,
                batch_size=self.config['batch_size'],
                shuffle=False,
                num_workers=4,
                pin_memory=True
            )
        else:
            self.val_loader = None
        
        logger.info(f"‚úÖ Data loaders created - Train: {len(self.train_loader)} batches")
    
    def train_epoch(self):
        """Train for one epoch."""
        self.model.train()
        total_loss = 0.0
        num_batches = 0
        
        # Set epoch for distributed sampler
        if hasattr(self.train_loader.sampler, 'set_epoch'):
            self.train_loader.sampler.set_epoch(self.epoch)
        
        for batch_idx, batch in enumerate(self.train_loader):
            start_time = time.time()
            
            # Move batch to device
            input_ids = batch['input_ids'].to(self.device)
            labels = batch['labels'].to(self.device)
            
            # Forward pass
            outputs = self.model(input_ids)
            logits = outputs['logits']
            
            # Compute loss
            loss = nn.CrossEntropyLoss()(
                logits.view(-1, logits.size(-1)),
                labels.view(-1)
            )
            
            # Backward pass
            self.optimizer.zero_grad()
            loss.backward()
            
            # Gradient clipping
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)
            
            # Optimizer step
            self.optimizer.step()
            self.scheduler.step()
            
            # Update metrics
            total_loss += loss.item()
            num_batches += 1
            self.global_step += 1
            
            # Logging
            if batch_idx % self.config['log_interval'] == 0:
                avg_loss = total_loss / num_batches
                lr = self.scheduler.get_last_lr()[0]
                batch_time = time.time() - start_time
                
                logger.info(
                    f"Epoch {self.epoch}, Step {batch_idx}/{len(self.train_loader)}, "
                    f"Loss: {loss.item():.4f}, Avg Loss: {avg_loss:.4f}, "
                    f"LR: {lr:.2e}, Time: {batch_time:.2f}s"
                )
                
                # Log to wandb
                if self.config.get('use_wandb', True) and dist.get_rank() == 0 and WANDB_AVAILABLE:
                    wandb.log({
                        'train_loss': loss.item(),
                        'avg_train_loss': avg_loss,
                        'learning_rate': lr,
                        'epoch': self.epoch,
                        'global_step': self.global_step
                    })
            
            # Checkpointing
            if self.global_step % self.config['checkpoint_interval'] == 0:
                self.save_checkpoint()
        
        return total_loss / num_batches
    
    def validate(self):
        """Run validation."""
        if self.val_loader is None:
            return None
        
        self.model.eval()
        total_loss = 0.0
        num_batches = 0
        
        with torch.no_grad():
            for batch in self.val_loader:
                input_ids = batch['input_ids'].to(self.device)
                labels = batch['labels'].to(self.device)
                
                outputs = self.model(input_ids)
                logits = outputs['logits']
                
                loss = nn.CrossEntropyLoss()(
                    logits.view(-1, logits.size(-1)),
                    labels.view(-1)
                )
                
                total_loss += loss.item()
                num_batches += 1
        
        avg_loss = total_loss / num_batches
        logger.info(f"Validation Loss: {avg_loss:.4f}")
        
        if self.config.get('use_wandb', True) and dist.get_rank() == 0:
            wandb.log({'val_loss': avg_loss, 'epoch': self.epoch})
        
        return avg_loss
    
    def save_checkpoint(self):
        """Save training checkpoint."""
        if dist.get_rank() == 0:  # Only save on rank 0
            checkpoint = {
                'epoch': self.epoch,
                'global_step': self.global_step,
                'model_state_dict': self.model.module.state_dict() if hasattr(self.model, 'module') else self.model.state_dict(),
                'optimizer_state_dict': self.optimizer.state_dict(),
                'scheduler_state_dict': self.scheduler.state_dict(),
                'config': self.config
            }
            
            checkpoint_path = f"checkpoints/governance_sentinel_13b_step_{self.global_step}.pt"
            os.makedirs('checkpoints', exist_ok=True)
            torch.save(checkpoint, checkpoint_path)
            
            logger.info(f"üíæ Checkpoint saved: {checkpoint_path}")
    
    def train(self):
        """Main training loop."""
        logger.info("üöÄ Starting 13B Governance Sentinel training...")
        logger.info(f"üìä Training for {self.config['num_epochs']} epochs")
        logger.info(f"üéØ Target: Revolutionary governance AI from scratch")
        
        start_time = time.time()
        
        for epoch in range(self.config['num_epochs']):
            self.epoch = epoch
            
            # Train epoch
            train_loss = self.train_epoch()
            
            # Validate
            val_loss = self.validate()
            
            # Log epoch results
            epoch_time = time.time() - start_time
            logger.info(
                f"‚úÖ Epoch {epoch} complete - "
                f"Train Loss: {train_loss:.4f}, "
                f"Val Loss: {val_loss:.4f if val_loss else 'N/A'}, "
                f"Time: {epoch_time/3600:.1f}h"
            )
            
            # Save best model
            if val_loss and val_loss < self.best_loss:
                self.best_loss = val_loss
                self.save_checkpoint()
        
        # Final save
        self.save_checkpoint()
        
        total_time = time.time() - start_time
        logger.info(f"üéâ Training complete! Total time: {total_time/3600:.1f} hours")
        logger.info("üèõÔ∏è Revolutionary governance AI trained from scratch!")

def load_config(config_path: str = 'config_13b.yaml') -> Dict:
    """Load training configuration."""
    default_config = {
        'learning_rate': 1e-4,
        'batch_size': 2,  # Small batch size for 13B model
        'num_epochs': 3,
        'max_sequence_length': 2048,
        'weight_decay': 0.01,
        'log_interval': 10,
        'checkpoint_interval': 1000,
        'steps_per_epoch': 1000,
        'use_wandb': True,
        'train_data_path': 'data/governance_sentinel_13b_train.json',
        'val_data_path': 'data/governance_sentinel_13b_val.json'
    }
    
    if os.path.exists(config_path):
        with open(config_path, 'r') as f:
            config = yaml.safe_load(f)
        # Merge with defaults
        default_config.update(config)
    
    return default_config

def main():
    """Main training function."""
    # Load configuration
    config = load_config()
    
    # Create trainer
    trainer = GovernanceTrainer(config)
    
    # Start training
    trainer.train()
    
    logger.info("üéØ 13B Governance Sentinel training complete!")
    logger.info("üèõÔ∏è Constitutional reasoning: TRAINED")
    logger.info("ü§î Emotional veritas: TRAINED")
    logger.info("üîß Tool cognition: TRAINED")
    logger.info("‚öñÔ∏è Bias detection: TRAINED")
    logger.info("üöÄ Revolutionary governance AI ready!")

if __name__ == "__main__":
    main()

