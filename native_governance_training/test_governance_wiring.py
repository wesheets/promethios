#!/usr/bin/env python3
"""
Governance Wiring Test Infrastructure

This module provides comprehensive testing for the governance wiring system.
It validates that all components are properly connected, real (not None),
and functioning correctly together.

Key Features:
- Component instantiation testing
- Integration verification testing
- Real vs fake metrics validation
- End-to-end governance pipeline testing
- Performance and reliability testing

This ensures the governance system is actually wired and working,
not just governance theater.

Codex Contract: v2025.05.21
Phase ID: 6.3
"""

import asyncio
import logging
import unittest
import pytest
from typing import Dict, Any, List, Optional
from datetime import datetime, timedelta
import json
import tempfile
import shutil
from pathlib import Path
import time
import uuid

# Import governance components for testing
from governance_component_factory import GovernanceComponentFactory
from governance_dependency_injector import GovernanceDependencyInjector, get_governance_injector
from governance_event_bus import GovernanceEventBus, GovernanceEvent, EventPriority
from governance_storage_backend import GovernanceStorageBackend, GovernanceRecord, StorageType
from governance_monitor import GovernanceMonitor, get_governance_monitor, get_real_governance_component
from extensions.trust_metrics_extension import GovernanceIntegratedTrustCalculator
from extensions.emotion_telemetry_extension import GovernanceIntegratedEmotionLogger

# Import original components for comparison
import sys
import os
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'phase_6_3_new', 'src'))

logger = logging.getLogger(__name__)

class GovernanceWiringTestSuite:
    """
    Comprehensive test suite for governance wiring validation.
    
    This class provides extensive testing to ensure the governance system
    is properly wired with real components, not fake metrics or None values.
    
    Test Categories:
    1. Component Instantiation Tests
    2. Integration Verification Tests
    3. Real vs Fake Metrics Tests
    4. End-to-End Pipeline Tests
    5. Performance and Reliability Tests
    
    Codex Contract: v2025.05.21
    Phase ID: 6.3
    """
    
    def __init__(self, test_data_dir: Optional[str] = None):
        """
        Initialize test suite.
        
        Args:
            test_data_dir: Directory for test data storage
        """
        self.test_data_dir = test_data_dir or tempfile.mkdtemp(prefix="governance_test_")
        self.test_results: Dict[str, Any] = {}
        self.test_start_time: Optional[datetime] = None
        self.test_end_time: Optional[datetime] = None
        
        # Test components
        self.component_factory: Optional[GovernanceComponentFactory] = None
        self.dependency_injector: Optional[GovernanceDependencyInjector] = None
        self.event_bus: Optional[GovernanceEventBus] = None
        self.storage_backend: Optional[GovernanceStorageBackend] = None
        self.governance_monitor: Optional[GovernanceMonitor] = None
        
        logger.info(f"GovernanceWiringTestSuite initialized with test data dir: {self.test_data_dir}")
    
    async def run_all_tests(self) -> Dict[str, Any]:
        """
        Run all governance wiring tests.
        
        Returns:
            Comprehensive test results
        """
        self.test_start_time = datetime.now()
        logger.info("Starting comprehensive governance wiring tests...")
        
        try:
            # Initialize test environment
            await self._setup_test_environment()
            
            # Run test categories
            test_categories = [
                ("component_instantiation", self._test_component_instantiation),
                ("integration_verification", self._test_integration_verification),
                ("real_vs_fake_metrics", self._test_real_vs_fake_metrics),
                ("end_to_end_pipeline", self._test_end_to_end_pipeline),
                ("performance_reliability", self._test_performance_reliability),
                ("backwards_compatibility", self._test_backwards_compatibility),
                ("error_handling", self._test_error_handling),
                ("monitoring_alerts", self._test_monitoring_alerts)\n            ]\n            \n            for category_name, test_method in test_categories:\n                logger.info(f\"Running {category_name} tests...\")\n                \n                try:\n                    category_results = await test_method()\n                    self.test_results[category_name] = {\n                        'status': 'passed',\n                        'results': category_results,\n                        'timestamp': datetime.now().isoformat()\n                    }\n                    logger.info(f\"{category_name} tests passed\")\n                    \n                except Exception as e:\n                    self.test_results[category_name] = {\n                        'status': 'failed',\n                        'error': str(e),\n                        'timestamp': datetime.now().isoformat()\n                    }\n                    logger.error(f\"{category_name} tests failed: {e}\")\n            \n            # Generate final report\n            final_report = await self._generate_final_report()\n            \n            self.test_end_time = datetime.now()\n            logger.info(\"Governance wiring tests completed\")\n            \n            return final_report\n            \n        except Exception as e:\n            logger.error(f\"Test suite execution failed: {e}\")\n            raise\n        \n        finally:\n            await self._cleanup_test_environment()\n    \n    async def _setup_test_environment(self):\n        \"\"\"Set up test environment.\"\"\"\n        logger.info(\"Setting up test environment...\")\n        \n        # Create test data directory\n        Path(self.test_data_dir).mkdir(parents=True, exist_ok=True)\n        \n        # Initialize core components for testing\n        self.component_factory = GovernanceComponentFactory()\n        await self.component_factory.initialize()\n        \n        self.dependency_injector = await get_governance_injector()\n        \n        self.event_bus = GovernanceEventBus(enable_self_verification=True)\n        await self.event_bus.start()\n        \n        self.storage_backend = GovernanceStorageBackend(\n            storage_type=StorageType.MEMORY,  # Use memory for testing\n            storage_path=self.test_data_dir\n        )\n        await self.storage_backend.initialize()\n        \n        self.governance_monitor = GovernanceMonitor()\n        await self.governance_monitor.initialize()\n        \n        logger.info(\"Test environment setup complete\")\n    \n    async def _cleanup_test_environment(self):\n        \"\"\"Clean up test environment.\"\"\"\n        logger.info(\"Cleaning up test environment...\")\n        \n        try:\n            # Stop monitoring\n            if self.governance_monitor:\n                await self.governance_monitor.stop_monitoring()\n            \n            # Stop event bus\n            if self.event_bus:\n                await self.event_bus.stop()\n            \n            # Shutdown storage\n            if self.storage_backend:\n                await self.storage_backend.shutdown()\n            \n            # Clean up test data directory\n            if os.path.exists(self.test_data_dir):\n                shutil.rmtree(self.test_data_dir)\n            \n            logger.info(\"Test environment cleanup complete\")\n            \n        except Exception as e:\n            logger.warning(f\"Error during test cleanup: {e}\")\n    \n    async def _test_component_instantiation(self) -> Dict[str, Any]:\n        \"\"\"Test that all components are properly instantiated (not None).\"\"\"\n        results = {\n            'total_components': 0,\n            'real_components': 0,\n            'none_components': 0,\n            'component_details': {},\n            'instantiation_success': False\n        }\n        \n        try:\n            # Get all components from dependency injector\n            components = await self.dependency_injector.get_all_components()\n            \n            results['total_components'] = len(components)\n            \n            for component_name, component in components.items():\n                is_real = component is not None\n                component_type = type(component).__name__ if component else 'NoneType'\n                \n                results['component_details'][component_name] = {\n                    'is_real': is_real,\n                    'type': component_type,\n                    'has_methods': self._check_component_methods(component) if is_real else []\n                }\n                \n                if is_real:\n                    results['real_components'] += 1\n                else:\n                    results['none_components'] += 1\n            \n            # Test passes if all components are real\n            results['instantiation_success'] = results['none_components'] == 0\n            \n            # Additional validation: Check specific expected components\n            expected_components = [\n                'trust_metrics_calculator',\n                'emotion_telemetry_logger',\n                'decision_framework_engine',\n                'governance_core',\n                'enhanced_veritas_engine'\n            ]\n            \n            missing_components = []\n            for expected in expected_components:\n                if expected not in components or components[expected] is None:\n                    missing_components.append(expected)\n            \n            results['missing_expected_components'] = missing_components\n            results['all_expected_present'] = len(missing_components) == 0\n            \n            return results\n            \n        except Exception as e:\n            results['error'] = str(e)\n            return results\n    \n    def _check_component_methods(self, component: Any) -> List[str]:\n        \"\"\"Check what methods a component has.\"\"\"\n        if component is None:\n            return []\n        \n        methods = []\n        for attr_name in dir(component):\n            if not attr_name.startswith('_'):\n                attr = getattr(component, attr_name)\n                if callable(attr):\n                    methods.append(attr_name)\n        \n        return methods\n    \n    async def _test_integration_verification(self) -> Dict[str, Any]:\n        \"\"\"Test that components can communicate with each other.\"\"\"\n        results = {\n            'integration_tests': {},\n            'successful_integrations': 0,\n            'failed_integrations': 0,\n            'integration_success_rate': 0.0\n        }\n        \n        try:\n            # Get all real components\n            components = await self.dependency_injector.get_all_components()\n            \n            # Test specific integrations\n            integration_tests = [\n                ('trust_metrics_calculator', 'emotion_telemetry_logger'),\n                ('trust_metrics_calculator', 'governance_core'),\n                ('decision_framework_engine', 'trust_metrics_calculator'),\n                ('enhanced_veritas_engine', 'trust_metrics_calculator')\n            ]\n            \n            for comp_a_name, comp_b_name in integration_tests:\n                test_name = f\"{comp_a_name}<->{comp_b_name}\"\n                \n                comp_a = components.get(comp_a_name)\n                comp_b = components.get(comp_b_name)\n                \n                if comp_a and comp_b:\n                    integration_result = await self._test_component_integration(comp_a, comp_b, comp_a_name, comp_b_name)\n                    results['integration_tests'][test_name] = integration_result\n                    \n                    if integration_result['success']:\n                        results['successful_integrations'] += 1\n                    else:\n                        results['failed_integrations'] += 1\n                else:\n                    results['integration_tests'][test_name] = {\n                        'success': False,\n                        'error': f\"Missing components: {comp_a_name}={comp_a is not None}, {comp_b_name}={comp_b is not None}\"\n                    }\n                    results['failed_integrations'] += 1\n            \n            total_tests = len(integration_tests)\n            results['integration_success_rate'] = results['successful_integrations'] / total_tests if total_tests > 0 else 0\n            \n            return results\n            \n        except Exception as e:\n            results['error'] = str(e)\n            return results\n    \n    async def _test_component_integration(self, comp_a: Any, comp_b: Any, name_a: str, name_b: str) -> Dict[str, Any]:\n        \"\"\"Test integration between two specific components.\"\"\"\n        result = {\n            'success': False,\n            'tests_performed': [],\n            'errors': []\n        }\n        \n        try:\n            # Test 1: Health check methods\n            if hasattr(comp_a, 'health_check') and hasattr(comp_b, 'health_check'):\n                try:\n                    health_a = await self._call_method_safely(comp_a.health_check)\n                    health_b = await self._call_method_safely(comp_b.health_check)\n                    \n                    result['tests_performed'].append('health_check')\n                    result['health_check_a'] = health_a\n                    result['health_check_b'] = health_b\n                    \n                except Exception as e:\n                    result['errors'].append(f\"Health check failed: {e}\")\n            \n            # Test 2: Method compatibility\n            common_methods = set(dir(comp_a)) & set(dir(comp_b))\n            callable_common_methods = [\n                method for method in common_methods \n                if not method.startswith('_') and callable(getattr(comp_a, method)) and callable(getattr(comp_b, method))\n            ]\n            \n            result['common_methods'] = callable_common_methods\n            result['tests_performed'].append('method_compatibility')\n            \n            # Test 3: Data exchange (if applicable)\n            if hasattr(comp_a, 'get_metrics') and hasattr(comp_b, 'process_metrics'):\n                try:\n                    metrics_a = await self._call_method_safely(comp_a.get_metrics)\n                    if metrics_a:\n                        process_result = await self._call_method_safely(comp_b.process_metrics, metrics_a)\n                        result['data_exchange_test'] = True\n                        result['tests_performed'].append('data_exchange')\n                except Exception as e:\n                    result['errors'].append(f\"Data exchange test failed: {e}\")\n            \n            # Integration is successful if no errors and at least one test performed\n            result['success'] = len(result['errors']) == 0 and len(result['tests_performed']) > 0\n            \n            return result\n            \n        except Exception as e:\n            result['errors'].append(str(e))\n            return result\n    \n    async def _call_method_safely(self, method, *args, **kwargs):\n        \"\"\"Call a method safely, handling both sync and async methods.\"\"\"\n        try:\n            if asyncio.iscoroutinefunction(method):\n                return await method(*args, **kwargs)\n            else:\n                return method(*args, **kwargs)\n        except Exception as e:\n            logger.warning(f\"Method call failed: {e}\")\n            return None\n    \n    async def _test_real_vs_fake_metrics(self) -> Dict[str, Any]:\n        \"\"\"Test that components generate real metrics, not fake ones.\"\"\"\n        results = {\n            'real_metrics_tests': {},\n            'fake_metrics_detected': [],\n            'real_metrics_confirmed': [],\n            'metrics_quality_score': 0.0\n        }\n        \n        try:\n            # Get governance components\n            components = await self.dependency_injector.get_all_components()\n            \n            # Test trust metrics calculator\n            trust_calc = components.get('trust_metrics_calculator')\n            if trust_calc:\n                trust_test = await self._test_trust_metrics_reality(trust_calc)\n                results['real_metrics_tests']['trust_calculator'] = trust_test\n                \n                if trust_test.get('is_real', False):\n                    results['real_metrics_confirmed'].append('trust_calculator')\n                else:\n                    results['fake_metrics_detected'].append('trust_calculator')\n            \n            # Test emotion telemetry logger\n            emotion_logger = components.get('emotion_telemetry_logger')\n            if emotion_logger:\n                emotion_test = await self._test_emotion_metrics_reality(emotion_logger)\n                results['real_metrics_tests']['emotion_logger'] = emotion_test\n                \n                if emotion_test.get('is_real', False):\n                    results['real_metrics_confirmed'].append('emotion_logger')\n                else:\n                    results['fake_metrics_detected'].append('emotion_logger')\n            \n            # Test enhanced veritas engine\n            veritas_engine = components.get('enhanced_veritas_engine')\n            if veritas_engine:\n                veritas_test = await self._test_veritas_metrics_reality(veritas_engine)\n                results['real_metrics_tests']['veritas_engine'] = veritas_test\n                \n                if veritas_test.get('is_real', False):\n                    results['real_metrics_confirmed'].append('veritas_engine')\n                else:\n                    results['fake_metrics_detected'].append('veritas_engine')\n            \n            # Calculate metrics quality score\n            total_components = len(results['real_metrics_tests'])\n            real_components = len(results['real_metrics_confirmed'])\n            results['metrics_quality_score'] = real_components / total_components if total_components > 0 else 0\n            \n            return results\n            \n        except Exception as e:\n            results['error'] = str(e)\n            return results\n    \n    async def _test_trust_metrics_reality(self, trust_calc: Any) -> Dict[str, Any]:\n        \"\"\"Test if trust metrics are real or fake.\"\"\"\n        result = {\n            'is_real': False,\n            'tests': {},\n            'evidence': []\n        }\n        \n        try:\n            # Test 1: Check for random.uniform usage (fake metrics indicator)\n            if hasattr(trust_calc, 'calculate_trust'):\n                # Call multiple times and check for variation patterns\n                trust_values = []\n                for _ in range(10):\n                    trust_value = await self._call_method_safely(trust_calc.calculate_trust)\n                    if trust_value is not None:\n                        trust_values.append(trust_value)\n                \n                result['tests']['multiple_calls'] = trust_values\n                \n                # Real metrics should show some consistency or pattern\n                # Fake metrics (random.uniform) show pure randomness\n                if len(trust_values) > 1:\n                    variance = self._calculate_variance(trust_values)\n                    result['tests']['variance'] = variance\n                    \n                    # If variance is too high, likely fake metrics\n                    if variance < 0.1:  # Low variance suggests real calculation\n                        result['evidence'].append('consistent_values')\n                    else:\n                        result['evidence'].append('high_variance_suggests_fake')\n            \n            # Test 2: Check for deterministic behavior with same inputs\n            if hasattr(trust_calc, 'calculate_trust_for_context'):\n                test_context = {'test': True, 'value': 42}\n                \n                result1 = await self._call_method_safely(trust_calc.calculate_trust_for_context, test_context)\n                result2 = await self._call_method_safely(trust_calc.calculate_trust_for_context, test_context)\n                \n                result['tests']['deterministic'] = {\n                    'result1': result1,\n                    'result2': result2,\n                    'same_result': result1 == result2\n                }\n                \n                if result1 == result2:\n                    result['evidence'].append('deterministic_behavior')\n                else:\n                    result['evidence'].append('non_deterministic_suggests_fake')\n            \n            # Test 3: Check for real calculation methods\n            real_methods = ['calculate_epistemic_uncertainty', 'calculate_aleatoric_uncertainty', 'get_trust_history']\n            found_real_methods = []\n            \n            for method_name in real_methods:\n                if hasattr(trust_calc, method_name):\n                    found_real_methods.append(method_name)\n            \n            result['tests']['real_methods'] = found_real_methods\n            \n            if found_real_methods:\n                result['evidence'].append('has_real_calculation_methods')\n            \n            # Determine if metrics are real based on evidence\n            real_evidence = ['consistent_values', 'deterministic_behavior', 'has_real_calculation_methods']\n            fake_evidence = ['high_variance_suggests_fake', 'non_deterministic_suggests_fake']\n            \n            real_score = sum(1 for evidence in result['evidence'] if evidence in real_evidence)\n            fake_score = sum(1 for evidence in result['evidence'] if evidence in fake_evidence)\n            \n            result['is_real'] = real_score > fake_score\n            result['confidence'] = abs(real_score - fake_score) / max(real_score + fake_score, 1)\n            \n            return result\n            \n        except Exception as e:\n            result['error'] = str(e)\n            return result\n    \n    def _calculate_variance(self, values: List[float]) -> float:\n        \"\"\"Calculate variance of a list of values.\"\"\"\n        if len(values) < 2:\n            return 0.0\n        \n        mean = sum(values) / len(values)\n        variance = sum((x - mean) ** 2 for x in values) / len(values)\n        return variance\n    \n    async def _test_emotion_metrics_reality(self, emotion_logger: Any) -> Dict[str, Any]:\n        \"\"\"Test if emotion metrics are real or fake.\"\"\"\n        result = {\n            'is_real': False,\n            'tests': {},\n            'evidence': []\n        }\n        \n        try:\n            # Test for real emotion analysis vs fake metrics\n            if hasattr(emotion_logger, 'analyze_emotion'):\n                test_texts = [\n                    \"I am very happy today!\",\n                    \"This is terrible and makes me angry.\",\n                    \"I feel neutral about this situation.\"\n                ]\n                \n                emotion_results = []\n                for text in test_texts:\n                    emotion_result = await self._call_method_safely(emotion_logger.analyze_emotion, text)\n                    emotion_results.append(emotion_result)\n                \n                result['tests']['emotion_analysis'] = emotion_results\n                \n                # Real emotion analysis should show different results for different texts\n                if len(set(str(r) for r in emotion_results)) > 1:\n                    result['evidence'].append('varied_emotion_responses')\n                else:\n                    result['evidence'].append('identical_responses_suggests_fake')\n            \n            # Test for real telemetry storage\n            if hasattr(emotion_logger, 'get_emotion_history'):\n                history = await self._call_method_safely(emotion_logger.get_emotion_history)\n                \n                result['tests']['has_history'] = history is not None\n                \n                if history:\n                    result['evidence'].append('has_emotion_history')\n            \n            # Determine reality based on evidence\n            real_evidence = ['varied_emotion_responses', 'has_emotion_history']\n            fake_evidence = ['identical_responses_suggests_fake']\n            \n            real_score = sum(1 for evidence in result['evidence'] if evidence in real_evidence)\n            fake_score = sum(1 for evidence in result['evidence'] if evidence in fake_evidence)\n            \n            result['is_real'] = real_score > fake_score\n            \n            return result\n            \n        except Exception as e:\n            result['error'] = str(e)\n            return result\n    \n    async def _test_veritas_metrics_reality(self, veritas_engine: Any) -> Dict[str, Any]:\n        \"\"\"Test if veritas metrics are real or fake.\"\"\"\n        result = {\n            'is_real': False,\n            'tests': {},\n            'evidence': []\n        }\n        \n        try:\n            # Test for real uncertainty quantification\n            if hasattr(veritas_engine, 'quantify_uncertainty'):\n                test_queries = [\n                    \"What is 2+2?\",  # High certainty\n                    \"What will the weather be like in 100 years?\",  # High uncertainty\n                    \"What is the capital of France?\"  # Medium certainty\n                ]\n                \n                uncertainty_results = []\n                for query in test_queries:\n                    uncertainty = await self._call_method_safely(veritas_engine.quantify_uncertainty, query)\n                    uncertainty_results.append(uncertainty)\n                \n                result['tests']['uncertainty_quantification'] = uncertainty_results\n                \n                # Real uncertainty should vary based on query difficulty\n                if len(set(str(r) for r in uncertainty_results)) > 1:\n                    result['evidence'].append('varied_uncertainty_responses')\n                else:\n                    result['evidence'].append('identical_uncertainty_suggests_fake')\n            \n            # Test for real self-questioning\n            if hasattr(veritas_engine, 'generate_self_questions'):\n                questions = await self._call_method_safely(veritas_engine.generate_self_questions, \"Test query\")\n                \n                result['tests']['self_questions'] = questions\n                \n                if questions and len(questions) > 0:\n                    result['evidence'].append('generates_self_questions')\n            \n            # Determine reality\n            real_evidence = ['varied_uncertainty_responses', 'generates_self_questions']\n            fake_evidence = ['identical_uncertainty_suggests_fake']\n            \n            real_score = sum(1 for evidence in result['evidence'] if evidence in real_evidence)\n            fake_score = sum(1 for evidence in result['evidence'] if evidence in fake_evidence)\n            \n            result['is_real'] = real_score > fake_score\n            \n            return result\n            \n        except Exception as e:\n            result['error'] = str(e)\n            return result\n    \n    async def _test_end_to_end_pipeline(self) -> Dict[str, Any]:\n        \"\"\"Test the complete governance pipeline end-to-end.\"\"\"\n        results = {\n            'pipeline_tests': {},\n            'pipeline_success': False,\n            'data_flow_verified': False\n        }\n        \n        try:\n            # Test complete governance decision pipeline\n            test_input = {\n                'query': 'Should I invest in cryptocurrency?',\n                'context': {'user_risk_tolerance': 'medium', 'investment_amount': 1000},\n                'timestamp': datetime.now().isoformat()\n            }\n            \n            # Step 1: Trust calculation\n            trust_calc = await self.governance_monitor.get_real_component('trust_metrics_calculator')\n            if trust_calc:\n                trust_result = await self._call_method_safely(trust_calc.calculate_trust_for_context, test_input)\n                results['pipeline_tests']['trust_calculation'] = {\n                    'success': trust_result is not None,\n                    'result': trust_result\n                }\n            \n            # Step 2: Emotion analysis\n            emotion_logger = await self.governance_monitor.get_real_component('emotion_telemetry_logger')\n            if emotion_logger:\n                emotion_result = await self._call_method_safely(emotion_logger.analyze_emotion, test_input['query'])\n                results['pipeline_tests']['emotion_analysis'] = {\n                    'success': emotion_result is not None,\n                    'result': emotion_result\n                }\n            \n            # Step 3: Decision framework\n            decision_engine = await self.governance_monitor.get_real_component('decision_framework_engine')\n            if decision_engine:\n                decision_result = await self._call_method_safely(decision_engine.make_decision, test_input)\n                results['pipeline_tests']['decision_making'] = {\n                    'success': decision_result is not None,\n                    'result': decision_result\n                }\n            \n            # Step 4: Veritas uncertainty\n            veritas_engine = await self.governance_monitor.get_real_component('enhanced_veritas_engine')\n            if veritas_engine:\n                uncertainty_result = await self._call_method_safely(veritas_engine.quantify_uncertainty, test_input['query'])\n                results['pipeline_tests']['uncertainty_quantification'] = {\n                    'success': uncertainty_result is not None,\n                    'result': uncertainty_result\n                }\n            \n            # Verify data flow between components\n            successful_steps = sum(1 for test in results['pipeline_tests'].values() if test.get('success', False))\n            total_steps = len(results['pipeline_tests'])\n            \n            results['pipeline_success'] = successful_steps == total_steps\n            results['success_rate'] = successful_steps / total_steps if total_steps > 0 else 0\n            results['data_flow_verified'] = successful_steps > 0\n            \n            return results\n            \n        except Exception as e:\n            results['error'] = str(e)\n            return results\n    \n    async def _test_performance_reliability(self) -> Dict[str, Any]:\n        \"\"\"Test performance and reliability of governance components.\"\"\"\n        results = {\n            'performance_tests': {},\n            'reliability_tests': {},\n            'performance_acceptable': False,\n            'reliability_acceptable': False\n        }\n        \n        try:\n            # Performance test: Response time\n            components = await self.governance_monitor.get_all_real_components()\n            \n            for component_name, component in components.items():\n                if hasattr(component, 'health_check'):\n                    # Measure response time\n                    start_time = time.time()\n                    health_result = await self._call_method_safely(component.health_check)\n                    end_time = time.time()\n                    \n                    response_time = end_time - start_time\n                    \n                    results['performance_tests'][component_name] = {\n                        'response_time_seconds': response_time,\n                        'health_check_success': health_result is not None,\n                        'acceptable_performance': response_time < 1.0  # 1 second threshold\n                    }\n            \n            # Reliability test: Multiple calls\n            trust_calc = components.get('trust_metrics_calculator')\n            if trust_calc and hasattr(trust_calc, 'calculate_trust'):\n                success_count = 0\n                total_calls = 10\n                \n                for i in range(total_calls):\n                    try:\n                        result = await self._call_method_safely(trust_calc.calculate_trust)\n                        if result is not None:\n                            success_count += 1\n                    except Exception:\n                        pass\n                \n                reliability_rate = success_count / total_calls\n                results['reliability_tests']['trust_calculator'] = {\n                    'success_rate': reliability_rate,\n                    'successful_calls': success_count,\n                    'total_calls': total_calls,\n                    'reliable': reliability_rate >= 0.9  # 90% success rate threshold\n                }\n            \n            # Overall assessment\n            performance_scores = [test.get('acceptable_performance', False) for test in results['performance_tests'].values()]\n            reliability_scores = [test.get('reliable', False) for test in results['reliability_tests'].values()]\n            \n            results['performance_acceptable'] = all(performance_scores) if performance_scores else False\n            results['reliability_acceptable'] = all(reliability_scores) if reliability_scores else False\n            \n            return results\n            \n        except Exception as e:\n            results['error'] = str(e)\n            return results\n    \n    async def _test_backwards_compatibility(self) -> Dict[str, Any]:\n        \"\"\"Test backwards compatibility with existing code.\"\"\"\n        results = {\n            'compatibility_tests': {},\n            'backwards_compatible': False\n        }\n        \n        try:\n            # Test that new components maintain same API as original components\n            components = await self.governance_monitor.get_all_real_components()\n            \n            # Test trust calculator API compatibility\n            trust_calc = components.get('trust_metrics_calculator')\n            if trust_calc:\n                expected_methods = ['calculate_trust', 'health_check']\n                available_methods = [method for method in expected_methods if hasattr(trust_calc, method)]\n                \n                results['compatibility_tests']['trust_calculator'] = {\n                    'expected_methods': expected_methods,\n                    'available_methods': available_methods,\n                    'api_compatible': len(available_methods) == len(expected_methods)\n                }\n            \n            # Test emotion logger API compatibility\n            emotion_logger = components.get('emotion_telemetry_logger')\n            if emotion_logger:\n                expected_methods = ['log_emotion', 'health_check']\n                available_methods = [method for method in expected_methods if hasattr(emotion_logger, method)]\n                \n                results['compatibility_tests']['emotion_logger'] = {\n                    'expected_methods': expected_methods,\n                    'available_methods': available_methods,\n                    'api_compatible': len(available_methods) == len(expected_methods)\n                }\n            \n            # Overall compatibility assessment\n            compatibility_scores = [test.get('api_compatible', False) for test in results['compatibility_tests'].values()]\n            results['backwards_compatible'] = all(compatibility_scores) if compatibility_scores else False\n            \n            return results\n            \n        except Exception as e:\n            results['error'] = str(e)\n            return results\n    \n    async def _test_error_handling(self) -> Dict[str, Any]:\n        \"\"\"Test error handling and recovery mechanisms.\"\"\"\n        results = {\n            'error_handling_tests': {},\n            'graceful_degradation': False,\n            'recovery_mechanisms': False\n        }\n        \n        try:\n            # Test component failure handling\n            components = await self.governance_monitor.get_all_real_components()\n            \n            # Test invalid input handling\n            trust_calc = components.get('trust_metrics_calculator')\n            if trust_calc and hasattr(trust_calc, 'calculate_trust_for_context'):\n                # Test with invalid input\n                invalid_inputs = [None, {}, \"invalid\", 12345]\n                \n                error_handling_results = []\n                for invalid_input in invalid_inputs:\n                    try:\n                        result = await self._call_method_safely(trust_calc.calculate_trust_for_context, invalid_input)\n                        error_handling_results.append({\n                            'input': str(invalid_input),\n                            'handled_gracefully': True,\n                            'result': result\n                        })\n                    except Exception as e:\n                        error_handling_results.append({\n                            'input': str(invalid_input),\n                            'handled_gracefully': False,\n                            'error': str(e)\n                        })\n                \n                results['error_handling_tests']['invalid_input_handling'] = error_handling_results\n                \n                # Check if errors were handled gracefully (no exceptions)\n                graceful_handling = all(result['handled_gracefully'] for result in error_handling_results)\n                results['graceful_degradation'] = graceful_handling\n            \n            # Test recovery mechanisms\n            if self.governance_monitor:\n                # Test component recovery\n                monitor_status = await self.governance_monitor.get_system_status()\n                \n                results['error_handling_tests']['monitor_status'] = monitor_status\n                results['recovery_mechanisms'] = monitor_status.get('configuration', {}).get('automatic_recovery', False)\n            \n            return results\n            \n        except Exception as e:\n            results['error'] = str(e)\n            return results\n    \n    async def _test_monitoring_alerts(self) -> Dict[str, Any]:\n        \"\"\"Test monitoring and alerting functionality.\"\"\"\n        results = {\n            'monitoring_tests': {},\n            'alert_system_working': False,\n            'monitoring_active': False\n        }\n        \n        try:\n            if self.governance_monitor:\n                # Test monitoring status\n                system_status = await self.governance_monitor.get_system_status()\n                \n                results['monitoring_tests']['system_status'] = system_status\n                results['monitoring_active'] = system_status.get('is_monitoring', False)\n                \n                # Test health check\n                health_check = await self.governance_monitor.health_check()\n                \n                results['monitoring_tests']['health_check'] = health_check\n                results['alert_system_working'] = health_check.get('status') == 'healthy'\n            \n            return results\n            \n        except Exception as e:\n            results['error'] = str(e)\n            return results\n    \n    async def _generate_final_report(self) -> Dict[str, Any]:\n        \"\"\"Generate final test report.\"\"\"\n        # Calculate overall scores\n        passed_categories = sum(1 for result in self.test_results.values() if result.get('status') == 'passed')\n        total_categories = len(self.test_results)\n        overall_success_rate = passed_categories / total_categories if total_categories > 0 else 0\n        \n        # Determine overall status\n        if overall_success_rate >= 0.9:\n            overall_status = 'excellent'\n        elif overall_success_rate >= 0.7:\n            overall_status = 'good'\n        elif overall_success_rate >= 0.5:\n            overall_status = 'acceptable'\n        else:\n            overall_status = 'needs_improvement'\n        \n        # Generate recommendations\n        recommendations = []\n        \n        for category, result in self.test_results.items():\n            if result.get('status') == 'failed':\n                recommendations.append(f\"Fix issues in {category}: {result.get('error', 'Unknown error')}\")\n        \n        # Check specific issues\n        if 'component_instantiation' in self.test_results:\n            inst_result = self.test_results['component_instantiation'].get('results', {})\n            if inst_result.get('none_components', 0) > 0:\n                recommendations.append(\"Replace None components with real instances\")\n        \n        if 'real_vs_fake_metrics' in self.test_results:\n            metrics_result = self.test_results['real_vs_fake_metrics'].get('results', {})\n            if metrics_result.get('fake_metrics_detected'):\n                recommendations.append(f\"Replace fake metrics in: {', '.join(metrics_result['fake_metrics_detected'])}\")\n        \n        # Final report\n        final_report = {\n            'test_summary': {\n                'start_time': self.test_start_time.isoformat() if self.test_start_time else None,\n                'end_time': self.test_end_time.isoformat() if self.test_end_time else None,\n                'duration_seconds': (self.test_end_time - self.test_start_time).total_seconds() if self.test_start_time and self.test_end_time else None,\n                'total_categories': total_categories,\n                'passed_categories': passed_categories,\n                'failed_categories': total_categories - passed_categories,\n                'overall_success_rate': overall_success_rate,\n                'overall_status': overall_status\n            },\n            'category_results': self.test_results,\n            'recommendations': recommendations,\n            'governance_wiring_status': {\n                'components_wired': overall_success_rate >= 0.8,\n                'real_metrics': 'real_vs_fake_metrics' in self.test_results and \n                               self.test_results['real_vs_fake_metrics'].get('status') == 'passed',\n                'integration_verified': 'integration_verification' in self.test_results and \n                                      self.test_results['integration_verification'].get('status') == 'passed',\n                'monitoring_active': 'monitoring_alerts' in self.test_results and \n                                   self.test_results['monitoring_alerts'].get('status') == 'passed'\n            },\n            'next_steps': self._generate_next_steps(overall_status, recommendations)\n        }\n        \n        return final_report\n    \n    def _generate_next_steps(self, overall_status: str, recommendations: List[str]) -> List[str]:\n        \"\"\"Generate next steps based on test results.\"\"\"\n        if overall_status == 'excellent':\n            return [\n                \"Governance wiring is complete and working excellently\",\n                \"Proceed with frontend integration\",\n                \"Begin production deployment preparation\"\n            ]\n        elif overall_status == 'good':\n            return [\n                \"Governance wiring is mostly complete\",\n                \"Address remaining issues: \" + \", \".join(recommendations[:3]),\n                \"Proceed with frontend integration after fixes\"\n            ]\n        elif overall_status == 'acceptable':\n            return [\n                \"Governance wiring has significant issues\",\n                \"Priority fixes needed: \" + \", \".join(recommendations[:5]),\n                \"Complete backend wiring before frontend integration\"\n            ]\n        else:\n            return [\n                \"Governance wiring is not functional\",\n                \"Major rework required\",\n                \"Focus on component instantiation and integration issues\",\n                \"Do not proceed with frontend integration until backend is fixed\"\n            ]\n\n# Convenience functions for running tests\nasync def run_governance_wiring_tests(test_data_dir: Optional[str] = None) -> Dict[str, Any]:\n    \"\"\"Run comprehensive governance wiring tests.\"\"\"\n    test_suite = GovernanceWiringTestSuite(test_data_dir)\n    return await test_suite.run_all_tests()\n\nasync def quick_wiring_check() -> Dict[str, Any]:\n    \"\"\"Quick check to see if governance wiring is working.\"\"\"\n    try:\n        # Quick component check\n        monitor = await get_governance_monitor()\n        components = await monitor.get_all_real_components()\n        \n        none_components = [name for name, comp in components.items() if comp is None]\n        real_components = [name for name, comp in components.items() if comp is not None]\n        \n        return {\n            'wiring_status': 'working' if len(none_components) == 0 else 'broken',\n            'total_components': len(components),\n            'real_components': len(real_components),\n            'none_components': len(none_components),\n            'none_component_list': none_components,\n            'real_component_list': real_components,\n            'timestamp': datetime.now().isoformat()\n        }\n        \n    except Exception as e:\n        return {\n            'wiring_status': 'error',\n            'error': str(e),\n            'timestamp': datetime.now().isoformat()\n        }\n\nif __name__ == \"__main__\":\n    # Run tests if script is executed directly\n    async def main():\n        print(\"Running Governance Wiring Tests...\")\n        \n        # Quick check first\n        quick_result = await quick_wiring_check()\n        print(f\"Quick Check: {quick_result}\")\n        \n        # Full test suite\n        if quick_result.get('wiring_status') != 'error':\n            full_results = await run_governance_wiring_tests()\n            print(f\"Full Test Results: {json.dumps(full_results, indent=2)}\")\n        else:\n            print(\"Skipping full tests due to quick check error\")\n    \n    asyncio.run(main())"

