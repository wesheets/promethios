# 13B Governance Sentinel Training Configuration
# Revolutionary native governance AI training settings

# Model Architecture
model:
  name: "GovernanceSentinel13B"
  hidden_size: 5120
  num_layers: 40
  num_attention_heads: 40
  intermediate_size: 20480
  vocab_size: 50257  # GPT-2 tokenizer size
  max_position_embeddings: 2048

# Training Parameters
training:
  learning_rate: 1e-4
  batch_size: 2  # Small batch for 13B model on 8x A100
  gradient_accumulation_steps: 8  # Effective batch size = 2 * 8 * 8 GPUs = 128
  num_epochs: 3
  max_sequence_length: 2048
  weight_decay: 0.01
  beta1: 0.9
  beta2: 0.95
  eps: 1e-8
  max_grad_norm: 1.0

# Learning Rate Schedule
scheduler:
  type: "cosine_with_warmup"
  warmup_ratio: 0.1
  min_lr_ratio: 0.1

# Data Configuration
data:
  train_data_path: "data/governance_sentinel_13b_train.json"
  val_data_path: "data/governance_sentinel_13b_val.json"
  num_workers: 4
  pin_memory: true
  
# Governance-Specific Settings
governance:
  constitutional_frameworks:
    - "us_constitution"
    - "universal_human_rights" 
    - "democratic_principles"
    - "rule_of_law"
    - "separation_of_powers"
    - "checks_and_balances"
  
  bias_categories:
    - "political_bias"
    - "cultural_bias"
    - "demographic_bias"
    - "confirmation_bias"
    - "availability_bias"
    - "anchoring_bias"
  
  tool_categories:
    - "search_information"
    - "analysis_evaluation"
    - "calculation_modeling"
    - "communication_coordination"
    - "documentation_records"
    - "validation_verification"
    - "monitoring_oversight"
    - "reporting_transparency"
    - "decision_support"
    - "collaboration_facilitation"

# Monitoring and Logging
monitoring:
  use_wandb: true
  project_name: "governance-sentinel-13b"
  log_interval: 10
  eval_interval: 500
  save_interval: 1000
  
# Checkpointing
checkpointing:
  save_dir: "checkpoints"
  keep_last_n: 3
  save_best: true
  
# Distributed Training
distributed:
  backend: "nccl"
  find_unused_parameters: false
  
# Hardware Optimization
optimization:
  mixed_precision: true
  gradient_checkpointing: true
  compile_model: false  # Set to true for PyTorch 2.0+
  
# Validation
validation:
  governance_scenarios: true
  constitutional_reasoning: true
  bias_detection: true
  emotional_veritas: true
  tool_integration: true

# Expected Training Metrics
expected_metrics:
  training_time_hours: 14
  final_train_loss: 2.5
  final_val_loss: 2.8
  constitutional_alignment_score: 0.85
  bias_detection_accuracy: 0.92
  
# Cost Estimation
cost_estimation:
  hardware: "8x A100 80GB"
  hourly_rate: 15.0  # USD per hour
  estimated_total_cost: 210.0  # USD
  cost_vs_traditional: "500x cheaper"

# Revolutionary Features
revolutionary_features:
  constitutional_anchoring: "Native constitutional reasoning embedded in every layer"
  emotional_veritas: "Genuine self-reflection and uncertainty acknowledgment"
  tool_embedded_cognition: "Tools as extensions of reasoning, not external utilities"
  bias_detection: "Real-time bias prevention, not post-hoc correction"
  governance_first: "Governance intelligence grown from constitutional DNA"
  
# Success Criteria
success_criteria:
  constitutional_consistency: "> 85%"
  bias_scores: "< 0.2 across all categories"
  uncertainty_calibration: "Appropriate doubt expression"
  tool_orchestration: "Multi-step governance workflows"
  stakeholder_consideration: "Inclusive perspective analysis"

