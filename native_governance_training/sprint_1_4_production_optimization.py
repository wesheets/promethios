#!/usr/bin/env python3
"""
Sprint 1.4: Production Integration and Optimization

This module optimizes the governance system for production deployment by:
- Fixing storage backend method signatures
- Optimizing performance and reducing overhead
- Adding production-ready error handling and monitoring
- Implementing proper configuration management
- Adding deployment readiness checks
- Optimizing component communication
- Adding production logging and metrics

Key Features:
- Production-ready governance component factory
- Optimized storage and event bus performance
- Comprehensive error handling and recovery
- Production configuration management
- Performance monitoring and metrics
- Deployment validation and health checks
"""

import os
import sys
import json
import asyncio
import logging
import uuid
import time
from datetime import datetime, timedelta
from typing import Dict, Any, Optional, List, Union
import yaml
from dataclasses import dataclass, asdict

# Add parent directory to path for imports
sys.path.append('../')

# Import governance infrastructure
from governance_storage_backend import GovernanceStorageBackend, StorageType
from governance_event_bus import GovernanceEventBus, GovernanceEvent, EventPriority
from component_health_monitor import ComponentHealthMonitor

# Import Sprint 1.3 components
from sprint_1_3_enhanced_veritas_integration import (
    Sprint13EnhancedGovernanceFactory,
    EnhancedVeritasIntegration,
    EmotionTelemetryIntegration
)

# Import mock components
from mock_components import MockEmotionTelemetryLogger, MockEnhancedVeritas

@dataclass
class ProductionMetrics:
    """Production metrics for governance system monitoring"""
    component_count: int
    real_component_count: int
    mock_component_count: int
    average_response_time: float
    error_rate: float
    uptime_percentage: float
    memory_usage_mb: float
    event_throughput: float
    storage_operations_per_second: float
    last_health_check: datetime
    
    def to_dict(self) -> Dict[str, Any]:
        """Convert metrics to dictionary for serialization"""
        metrics_dict = asdict(self)
        metrics_dict['last_health_check'] = self.last_health_check.isoformat()
        return metrics_dict

class ProductionGovernanceStorageBackend:
    """
    Production-optimized storage backend with proper method signatures
    and performance optimizations.
    """
    
    def __init__(self, storage_type: StorageType = StorageType.MEMORY, 
                 storage_path: Optional[str] = None, 
                 retention_days: int = 30,
                 max_cache_size: int = 10000,
                 batch_size: int = 100):
        self.storage_type = storage_type
        self.storage_path = storage_path
        self.retention_days = retention_days
        self.max_cache_size = max_cache_size
        self.batch_size = batch_size
        
        # Performance tracking
        self.operation_count = 0
        self.total_operation_time = 0.0
        self.error_count = 0
        
        # In-memory cache for performance
        self.cache = {}
        self.cache_hits = 0
        self.cache_misses = 0
        
        # Initialize storage
        self.storage = {}
        self.logger = self._setup_logging()
        
    def _setup_logging(self) -> logging.Logger:
        """Setup production logging"""
        logger = logging.getLogger("ProductionGovernanceStorage")
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        
        return logger
    
    async def initialize(self):
        """Initialize production storage backend"""
        start_time = time.time()
        
        try:
            if self.storage_type == StorageType.FILE and self.storage_path:
                os.makedirs(os.path.dirname(self.storage_path), exist_ok=True)
                
            self.logger.info(f"Production storage backend initialized: {self.storage_type.value}")
            
        except Exception as e:
            self.error_count += 1
            self.logger.error(f"Failed to initialize storage backend: {e}")
            raise
        
        finally:
            self.total_operation_time += time.time() - start_time
            self.operation_count += 1
    
    async def store_data(self, data_type: str, data: Dict[str, Any], 
                        record_id: Optional[str] = None) -> str:
        """
        Store data with production-optimized method signature
        (Fixed method name from store_record to store_data)
        """
        start_time = time.time()
        
        try:
            # Input validation
            if data_type is None or data_type == "":
                raise ValueError("data_type cannot be None or empty")
            if data is None:
                raise ValueError("data cannot be None")
            if not isinstance(data, dict):
                raise ValueError("data must be a dictionary")
            
            if record_id is None:
                record_id = str(uuid.uuid4())
            
            # Add metadata
            record = {
                'id': record_id,
                'type': data_type,
                'data': data,
                'timestamp': datetime.now().isoformat(),
                'version': '1.0'
            }
            
            # Store in memory (production would use actual database)
            if data_type not in self.storage:
                self.storage[data_type] = {}
            
            self.storage[data_type][record_id] = record
            
            # Update cache
            cache_key = f"{data_type}:{record_id}"
            self.cache[cache_key] = record
            
            # Manage cache size
            if len(self.cache) > self.max_cache_size:
                # Remove oldest entries
                oldest_keys = list(self.cache.keys())[:len(self.cache) - self.max_cache_size + 1]
                for key in oldest_keys:
                    del self.cache[key]
            
            return record_id
            
        except Exception as e:
            self.error_count += 1
            self.logger.error(f"Failed to store data: {e}")
            raise
        
        finally:
            self.total_operation_time += time.time() - start_time
            self.operation_count += 1
    
    async def retrieve_data(self, data_type: str, record_id: str) -> Optional[Dict[str, Any]]:
        """Retrieve data with caching optimization"""
        start_time = time.time()
        
        try:
            cache_key = f"{data_type}:{record_id}"
            
            # Check cache first
            if cache_key in self.cache:
                self.cache_hits += 1
                return self.cache[cache_key]
            
            self.cache_misses += 1
            
            # Retrieve from storage
            if data_type in self.storage and record_id in self.storage[data_type]:
                record = self.storage[data_type][record_id]
                
                # Update cache
                self.cache[cache_key] = record
                
                return record
            
            return None
            
        except Exception as e:
            self.error_count += 1
            self.logger.error(f"Failed to retrieve data: {e}")
            return None
        
        finally:
            self.total_operation_time += time.time() - start_time
            self.operation_count += 1
    
    async def query_data(self, data_type: str, filters: Optional[Dict[str, Any]] = None,
                        limit: int = 100, offset: int = 0) -> List[Dict[str, Any]]:
        """Query data with pagination and filtering"""
        start_time = time.time()
        
        try:
            if data_type not in self.storage:
                return []
            
            records = list(self.storage[data_type].values())
            
            # Apply filters
            if filters:
                filtered_records = []
                for record in records:
                    match = True
                    for key, value in filters.items():
                        if key not in record['data'] or record['data'][key] != value:
                            match = False
                            break
                    if match:
                        filtered_records.append(record)
                records = filtered_records
            
            # Apply pagination
            start_idx = offset
            end_idx = offset + limit
            
            return records[start_idx:end_idx]
            
        except Exception as e:
            self.error_count += 1
            self.logger.error(f"Failed to query data: {e}")
            return []
        
        finally:
            self.total_operation_time += time.time() - start_time
            self.operation_count += 1
    
    async def cleanup_old_records(self) -> int:
        """Clean up old records based on retention policy"""
        start_time = time.time()
        cleaned_count = 0
        
        try:
            cutoff_date = datetime.now() - timedelta(days=self.retention_days)
            
            for data_type in list(self.storage.keys()):
                records_to_remove = []
                
                for record_id, record in self.storage[data_type].items():
                    record_date = datetime.fromisoformat(record['timestamp'])
                    if record_date < cutoff_date:
                        records_to_remove.append(record_id)
                
                for record_id in records_to_remove:
                    del self.storage[data_type][record_id]
                    
                    # Remove from cache
                    cache_key = f"{data_type}:{record_id}"
                    if cache_key in self.cache:
                        del self.cache[cache_key]
                    
                    cleaned_count += 1
            
            if cleaned_count > 0:
                self.logger.info(f"Cleaned up {cleaned_count} old records")
            
            return cleaned_count
            
        except Exception as e:
            self.error_count += 1
            self.logger.error(f"Failed to cleanup old records: {e}")
            return 0
        
        finally:
            self.total_operation_time += time.time() - start_time
            self.operation_count += 1
    
    def get_performance_metrics(self) -> Dict[str, Any]:
        """Get storage performance metrics"""
        avg_operation_time = (
            self.total_operation_time / self.operation_count 
            if self.operation_count > 0 else 0
        )
        
        cache_hit_rate = (
            self.cache_hits / (self.cache_hits + self.cache_misses)
            if (self.cache_hits + self.cache_misses) > 0 else 0
        )
        
        error_rate = (
            self.error_count / self.operation_count
            if self.operation_count > 0 else 0
        )
        
        return {
            'total_operations': self.operation_count,
            'average_operation_time_ms': avg_operation_time * 1000,
            'operations_per_second': 1 / avg_operation_time if avg_operation_time > 0 else 0,
            'cache_hit_rate': cache_hit_rate,
            'cache_size': len(self.cache),
            'error_rate': error_rate,
            'total_records': sum(len(records) for records in self.storage.values())
        }
    
    async def shutdown(self):
        """Graceful shutdown with cleanup"""
        try:
            # Perform final cleanup
            await self.cleanup_old_records()
            
            # Clear cache
            self.cache.clear()
            
            self.logger.info("Production storage backend shutdown complete")
            
        except Exception as e:
            self.logger.error(f"Error during storage shutdown: {e}")


class ProductionGovernanceEventBus:
    """
    Production-optimized event bus with performance monitoring
    and error handling.
    """
    
    def __init__(self, max_queue_size: int = 10000,
                 batch_processing_size: int = 50,
                 processing_interval: float = 0.1):
        self.max_queue_size = max_queue_size
        self.batch_processing_size = batch_processing_size
        self.processing_interval = processing_interval
        
        # Event queues
        self.event_queue = asyncio.Queue(maxsize=max_queue_size)
        self.subscribers = {}
        
        # Performance tracking
        self.events_published = 0
        self.events_processed = 0
        self.processing_errors = 0
        self.total_processing_time = 0.0
        
        # Processing control
        self.is_running = False
        self.processor_task = None
        
        self.logger = self._setup_logging()
    
    def _setup_logging(self) -> logging.Logger:
        """Setup production logging"""
        logger = logging.getLogger("ProductionGovernanceEventBus")
        logger.setLevel(logging.INFO)
        
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        
        return logger
    
    async def start(self):
        """Start the event processing loop"""
        if not self.is_running:
            self.is_running = True
            self.processor_task = asyncio.create_task(self._process_events())
            self.logger.info("Production event bus started")
    
    async def stop(self):
        """Stop the event processing loop"""
        if self.is_running:
            self.is_running = False
            if self.processor_task:
                self.processor_task.cancel()
                try:
                    await self.processor_task
                except asyncio.CancelledError:
                    pass
            self.logger.info("Production event bus stopped")
    
    async def publish(self, event: GovernanceEvent) -> bool:
        """Publish event with production error handling"""
        # Input validation - raise exceptions for invalid input
        if event is None:
            raise ValueError("event cannot be None")
        if not isinstance(event, GovernanceEvent):
            raise ValueError("event must be a GovernanceEvent instance")
        
        try:
            if self.event_queue.full():
                self.logger.warning("Event queue is full, dropping oldest event")
                try:
                    self.event_queue.get_nowait()
                except asyncio.QueueEmpty:
                    pass
            
            await self.event_queue.put(event)
            self.events_published += 1
            
            return True
            
        except Exception as e:
            self.processing_errors += 1
            self.logger.error(f"Failed to publish event: {e}")
            raise  # Re-raise the exception instead of returning False
    
    async def _process_events(self):
        """Process events in batches for better performance"""
        while self.is_running:
            try:
                events_batch = []
                
                # Collect batch of events
                for _ in range(self.batch_processing_size):
                    try:
                        event = await asyncio.wait_for(
                            self.event_queue.get(), 
                            timeout=self.processing_interval
                        )
                        events_batch.append(event)
                    except asyncio.TimeoutError:
                        break
                
                if events_batch:
                    await self._process_event_batch(events_batch)
                
            except Exception as e:
                self.processing_errors += 1
                self.logger.error(f"Error in event processing loop: {e}")
                await asyncio.sleep(1)  # Brief pause before retrying
    
    async def _process_event_batch(self, events: List[GovernanceEvent]):
        """Process a batch of events"""
        start_time = time.time()
        
        try:
            for event in events:
                # Process each event
                await self._process_single_event(event)
                self.events_processed += 1
            
        except Exception as e:
            self.processing_errors += 1
            self.logger.error(f"Error processing event batch: {e}")
        
        finally:
            self.total_processing_time += time.time() - start_time
    
    async def _process_single_event(self, event: GovernanceEvent):
        """Process a single event"""
        try:
            # Log high priority events
            if event.priority in [EventPriority.CRITICAL, EventPriority.HIGH]:
                self.logger.warning(f"High priority event: {event.type} from {event.source_component}")
            
            # Process event based on type
            if event.type == 'component_health_check':
                await self._handle_health_check_event(event)
            elif event.type == 'governance_violation':
                await self._handle_governance_violation_event(event)
            elif event.type == 'performance_alert':
                await self._handle_performance_alert_event(event)
            
        except Exception as e:
            self.logger.error(f"Error processing event {event.id}: {e}")
    
    async def _handle_health_check_event(self, event: GovernanceEvent):
        """Handle component health check events"""
        component_name = event.source_component
        health_status = event.data.get('status', 'unknown')
        
        if health_status == 'unhealthy':
            self.logger.warning(f"Component {component_name} reported unhealthy status")
    
    async def _handle_governance_violation_event(self, event: GovernanceEvent):
        """Handle governance violation events"""
        violation_type = event.data.get('violation_type', 'unknown')
        severity = event.data.get('severity', 'medium')
        
        self.logger.error(f"Governance violation: {violation_type} (severity: {severity})")
    
    async def _handle_performance_alert_event(self, event: GovernanceEvent):
        """Handle performance alert events"""
        metric_name = event.data.get('metric', 'unknown')
        threshold = event.data.get('threshold', 'unknown')
        current_value = event.data.get('current_value', 'unknown')
        
        self.logger.warning(f"Performance alert: {metric_name} = {current_value} (threshold: {threshold})")
    
    def get_performance_metrics(self) -> Dict[str, Any]:
        """Get event bus performance metrics"""
        avg_processing_time = (
            self.total_processing_time / self.events_processed
            if self.events_processed > 0 else 0
        )
        
        error_rate = (
            self.processing_errors / (self.events_published + self.processing_errors)
            if (self.events_published + self.processing_errors) > 0 else 0
        )
        
        throughput = (
            self.events_processed / self.total_processing_time
            if self.total_processing_time > 0 else 0
        )
        
        return {
            'events_published': self.events_published,
            'events_processed': self.events_processed,
            'events_pending': self.event_queue.qsize(),
            'average_processing_time_ms': avg_processing_time * 1000,
            'events_per_second': throughput,
            'error_rate': error_rate,
            'queue_utilization': self.event_queue.qsize() / self.max_queue_size
        }


class ProductionGovernanceFactory(Sprint13EnhancedGovernanceFactory):
    """
    Production-ready governance factory with optimizations,
    monitoring, and deployment readiness checks.
    """
    
    def __init__(self, config_path: str = "production_governance_config.yaml"):
        super().__init__(config_path)
        
        # Production-specific attributes
        self.production_metrics = None
        self.health_check_interval = 30  # seconds
        self.performance_monitoring_enabled = True
        self.deployment_checks_enabled = True
        
        # Performance tracking
        self.component_creation_times = {}
        self.component_health_status = {}
        
        self.logger.info("Production Governance Factory initialized")
    
    async def create_all_components(self) -> Dict[str, Any]:
        """Create all components with production optimizations"""
        start_time = time.time()
        
        self.logger.info("Creating production governance components...")
        
        # Initialize production infrastructure
        await self._initialize_production_infrastructure()
        
        # Create components with timing
        components = {}
        
        # Create components one by one with timing
        component_creators = [
            ('trust_calculator', self._create_trust_metrics_calculator),
            ('emotion_telemetry_logger', self._create_emotion_telemetry_logger),
            ('decision_framework_engine', self._create_decision_framework_engine),
            ('enhanced_veritas_2', self._create_enhanced_veritas),
            ('governance_core', self._create_governance_core),
            ('reflection_loop_tracker', self._create_reflection_loop_tracker)
        ]
        
        for component_name, creator_func in component_creators:
            component_start = time.time()
            
            try:
                component = await creator_func()
                components[component_name] = component
                
                creation_time = time.time() - component_start
                self.component_creation_times[component_name] = creation_time
                self.component_health_status[component_name] = 'healthy'
                
                self.logger.info(f"‚úÖ {component_name} created in {creation_time:.3f}s")
                
            except Exception as e:
                self.logger.error(f"‚ùå Failed to create {component_name}: {e}")
                self.component_health_status[component_name] = 'failed'
                components[component_name] = None
        
        # Add infrastructure components
        components.update({
            'storage_backend': self.storage_backend,
            'event_bus': self.event_bus,
            'health_monitor': self.health_monitor
        })
        
        # Initialize Sprint 1.3 integrations
        await self._initialize_sprint_1_3_integrations()
        components.update({
            'enhanced_veritas_integration': self.enhanced_veritas_integration,
            'emotion_telemetry_integration': self.emotion_telemetry_integration
        })
        
        # Setup production monitoring
        await self._setup_production_monitoring(components)
        
        # Perform deployment readiness checks
        if self.deployment_checks_enabled:
            await self._perform_deployment_checks(components)
        
        # Calculate production metrics
        total_time = time.time() - start_time
        self.production_metrics = await self._calculate_production_metrics(components, total_time)
        
        self.components = components
        
        # Log production summary
        self.logger.info(f"‚úÖ Production governance system created in {total_time:.3f}s")
        self.logger.info(f"   Total components: {len(components)}")
        self.logger.info(f"   Real components: {self.real_components_count}")
        self.logger.info(f"   Mock components: {self.mock_components_count}")
        self.logger.info(f"   Average creation time: {sum(self.component_creation_times.values()) / len(self.component_creation_times):.3f}s")
        
        return components
    
    async def _initialize_production_infrastructure(self):
        """Initialize production-optimized infrastructure"""
        # Use production storage backend
        self.storage_backend = ProductionGovernanceStorageBackend(
            storage_type=StorageType.MEMORY,  # In production, would use PostgreSQL
            retention_days=self.config['storage']['retention_days'],
            max_cache_size=10000,
            batch_size=100
        )
        await self.storage_backend.initialize()
        
        # Use production event bus
        self.event_bus = ProductionGovernanceEventBus(
            max_queue_size=self.config['event_bus']['max_events'],
            batch_processing_size=50,
            processing_interval=0.1
        )
        await self.event_bus.start()
        
        # Enhanced health monitor
        self.health_monitor = {
            'status': 'healthy',
            'components': {},
            'check_interval': self.health_check_interval,
            'last_check': datetime.now(),
            'performance_metrics': {},
            'alerts': []
        }
        
        self.logger.info("‚úÖ Production infrastructure initialized")
    
    async def _setup_production_monitoring(self, components: Dict[str, Any]):
        """Setup production monitoring and health checks"""
        if not self.performance_monitoring_enabled:
            return
        
        # Setup component health monitoring
        for name, component in components.items():
            if component:
                self.health_monitor['components'][name] = {
                    'status': self.component_health_status.get(name, 'healthy'),
                    'last_check': datetime.now().isoformat(),
                    'component_type': component.__class__.__name__,
                    'creation_time': self.component_creation_times.get(name, 0)
                }
        
        # Start health check task
        asyncio.create_task(self._periodic_health_checks())
        
        self.logger.info("‚úÖ Production monitoring setup complete")
    
    async def _periodic_health_checks(self):
        """Perform periodic health checks"""
        while True:
            try:
                await asyncio.sleep(self.health_check_interval)
                
                # Check component health
                for name, component in self.components.items():
                    if component and hasattr(component, 'get_health_status'):
                        try:
                            health_status = await component.get_health_status()
                            self.health_monitor['components'][name]['status'] = health_status
                        except Exception as e:
                            self.health_monitor['components'][name]['status'] = 'unhealthy'
                            self.logger.warning(f"Health check failed for {name}: {e}")
                
                # Update performance metrics
                if hasattr(self.storage_backend, 'get_performance_metrics'):
                    self.health_monitor['performance_metrics']['storage'] = self.storage_backend.get_performance_metrics()
                
                if hasattr(self.event_bus, 'get_performance_metrics'):
                    self.health_monitor['performance_metrics']['event_bus'] = self.event_bus.get_performance_metrics()
                
                self.health_monitor['last_check'] = datetime.now()
                
            except Exception as e:
                self.logger.error(f"Error in periodic health checks: {e}")
    
    async def _perform_deployment_checks(self, components: Dict[str, Any]):
        """Perform deployment readiness checks"""
        checks = []
        
        # Check component availability
        required_components = [
            'trust_calculator', 'governance_core', 'storage_backend', 'event_bus'
        ]
        
        for component_name in required_components:
            if component_name in components and components[component_name] is not None:
                checks.append(f"‚úÖ {component_name}: Available")
            else:
                checks.append(f"‚ùå {component_name}: Missing")
        
        # Check infrastructure health
        if hasattr(self.storage_backend, 'get_performance_metrics'):
            storage_metrics = self.storage_backend.get_performance_metrics()
            if storage_metrics['error_rate'] < 0.01:  # Less than 1% error rate
                checks.append("‚úÖ Storage: Healthy")
            else:
                checks.append(f"‚ö†Ô∏è  Storage: High error rate ({storage_metrics['error_rate']:.2%})")
        
        if hasattr(self.event_bus, 'get_performance_metrics'):
            event_metrics = self.event_bus.get_performance_metrics()
            if event_metrics['error_rate'] < 0.01:  # Less than 1% error rate
                checks.append("‚úÖ Event Bus: Healthy")
            else:
                checks.append(f"‚ö†Ô∏è  Event Bus: High error rate ({event_metrics['error_rate']:.2%})")
        
        # Log deployment readiness
        self.logger.info("üöÄ Deployment Readiness Checks:")
        for check in checks:
            self.logger.info(f"   {check}")
        
        # Determine overall readiness
        failed_checks = [check for check in checks if check.startswith("‚ùå")]
        if failed_checks:
            self.logger.warning(f"‚ö†Ô∏è  Deployment readiness: {len(failed_checks)} critical issues found")
        else:
            self.logger.info("‚úÖ Deployment readiness: All checks passed")
    
    async def _calculate_production_metrics(self, components: Dict[str, Any], 
                                          total_creation_time: float) -> ProductionMetrics:
        """Calculate comprehensive production metrics"""
        
        # Component counts
        total_components = len([c for c in components.values() if c is not None])
        real_components = self.real_components_count
        mock_components = self.mock_components_count
        
        # Performance metrics
        avg_response_time = sum(self.component_creation_times.values()) / len(self.component_creation_times)
        
        # Error rate calculation
        failed_components = len([status for status in self.component_health_status.values() if status == 'failed'])
        error_rate = failed_components / len(self.component_health_status) if self.component_health_status else 0
        
        # Uptime (assume 100% for new deployment)
        uptime_percentage = 100.0
        
        # Memory usage (simplified calculation)
        memory_usage_mb = len(components) * 10  # Rough estimate
        
        # Event throughput
        event_throughput = 0.0
        if hasattr(self.event_bus, 'get_performance_metrics'):
            event_metrics = self.event_bus.get_performance_metrics()
            event_throughput = event_metrics.get('events_per_second', 0.0)
        
        # Storage operations per second
        storage_ops_per_second = 0.0
        if hasattr(self.storage_backend, 'get_performance_metrics'):
            storage_metrics = self.storage_backend.get_performance_metrics()
            storage_ops_per_second = storage_metrics.get('operations_per_second', 0.0)
        
        return ProductionMetrics(
            component_count=total_components,
            real_component_count=real_components,
            mock_component_count=mock_components,
            average_response_time=avg_response_time,
            error_rate=error_rate,
            uptime_percentage=uptime_percentage,
            memory_usage_mb=memory_usage_mb,
            event_throughput=event_throughput,
            storage_operations_per_second=storage_ops_per_second,
            last_health_check=datetime.now()
        )
    
    async def get_production_status(self) -> Dict[str, Any]:
        """Get comprehensive production status"""
        status = await self.get_component_status()
        
        # Add production-specific metrics
        if self.production_metrics:
            status['production_metrics'] = self.production_metrics.to_dict()
        
        # Add performance metrics
        if hasattr(self.storage_backend, 'get_performance_metrics'):
            status['storage_performance'] = self.storage_backend.get_performance_metrics()
        
        if hasattr(self.event_bus, 'get_performance_metrics'):
            status['event_bus_performance'] = self.event_bus.get_performance_metrics()
        
        # Add health monitor data
        status['health_monitor'] = self.health_monitor
        
        # Add deployment readiness
        status['deployment_ready'] = (
            self.production_metrics.error_rate < 0.01 if self.production_metrics else False
        )
        
        return status
    
    async def shutdown(self):
        """Production-ready shutdown with cleanup"""
        self.logger.info("Shutting down production governance system...")
        
        try:
            # Stop event bus
            if hasattr(self.event_bus, 'stop'):
                await self.event_bus.stop()
            
            # Shutdown storage
            if hasattr(self.storage_backend, 'shutdown'):
                await self.storage_backend.shutdown()
            
            # Shutdown components
            await super().shutdown()
            
            self.logger.info("‚úÖ Production governance system shutdown complete")
            
        except Exception as e:
            self.logger.error(f"Error during production shutdown: {e}")


# Test function for Sprint 1.4 validation
async def test_production_governance_system():
    """Test the production governance system"""
    print("üöÄ Testing Sprint 1.4: Production Integration and Optimization")
    print("=" * 80)
    
    factory = ProductionGovernanceFactory()
    
    try:
        # Create production system
        components = await factory.create_all_components()
        
        # Get production status
        status = await factory.get_production_status()
        
        print(f"\nüìä Production System Status:")
        print(f"   Total components: {status['total_components']}")
        print(f"   Real components: {status['real_components']}")
        print(f"   Mock components: {status['mock_components']}")
        print(f"   Deployment ready: {status['deployment_ready']}")
        
        if 'production_metrics' in status:
            metrics = status['production_metrics']
            print(f"\nüìà Production Metrics:")
            print(f"   Average response time: {metrics['average_response_time']:.3f}s")
            print(f"   Error rate: {metrics['error_rate']:.2%}")
            print(f"   Uptime: {metrics['uptime_percentage']:.1f}%")
            print(f"   Memory usage: {metrics['memory_usage_mb']:.1f} MB")
            print(f"   Event throughput: {metrics['event_throughput']:.1f} events/sec")
            print(f"   Storage ops/sec: {metrics['storage_operations_per_second']:.1f}")
        
        if 'storage_performance' in status:
            storage = status['storage_performance']
            print(f"\nüíæ Storage Performance:")
            print(f"   Operations: {storage['total_operations']}")
            print(f"   Avg operation time: {storage['average_operation_time_ms']:.2f}ms")
            print(f"   Cache hit rate: {storage['cache_hit_rate']:.2%}")
            print(f"   Error rate: {storage['error_rate']:.2%}")
        
        if 'event_bus_performance' in status:
            events = status['event_bus_performance']
            print(f"\nüì° Event Bus Performance:")
            print(f"   Events published: {events['events_published']}")
            print(f"   Events processed: {events['events_processed']}")
            print(f"   Events pending: {events['events_pending']}")
            print(f"   Processing rate: {events['events_per_second']:.1f} events/sec")
            print(f"   Queue utilization: {events['queue_utilization']:.2%}")
        
        # Test production functionality
        print(f"\nüß™ Testing Production Functionality:")
        
        # Test storage with correct method signature
        storage = components.get('storage_backend')
        if storage:
            try:
                record_id = await storage.store_data(
                    data_type='test_record',
                    data={'test': 'production_data', 'timestamp': datetime.now().isoformat()}
                )
                retrieved = await storage.retrieve_data('test_record', record_id)
                print(f"   ‚úÖ Storage: Record stored and retrieved successfully")
            except Exception as e:
                print(f"   ‚ö†Ô∏è  Storage error: {e}")
        
        # Test event bus
        event_bus = components.get('event_bus')
        if event_bus:
            try:
                test_event = GovernanceEvent(
                    id=str(uuid.uuid4()),
                    type='test_event',
                    source_component='test_component',
                    target_component=None,
                    timestamp=datetime.now(),
                    priority=EventPriority.LOW,
                    data={'test': 'production_event'}
                )
                success = await event_bus.publish(test_event)
                print(f"   ‚úÖ Event Bus: Event published successfully")
            except Exception as e:
                print(f"   ‚ö†Ô∏è  Event Bus error: {e}")
        
        print(f"\nüéâ Sprint 1.4 Production Test Complete!")
        print(f"‚úÖ Production-ready governance system with optimized performance")
        print(f"‚úÖ Comprehensive monitoring and health checks")
        print(f"‚úÖ Deployment readiness validation")
        print(f"‚úÖ Error handling and recovery mechanisms")
        
        return True
        
    except Exception as e:
        print(f"‚ùå Sprint 1.4 test failed: {e}")
        return False
    
    finally:
        await factory.shutdown()


if __name__ == "__main__":
    asyncio.run(test_production_governance_system())

